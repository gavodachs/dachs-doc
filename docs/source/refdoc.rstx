========================================
GAVO DC Software Reference Documentation
========================================

:Author: Markus Demleitner
:Email: gavo@ari.uni-heidelberg.de
:Date: |date|

.. contents:: 
  :depth: 2
  :backlinks: entry
  :class: toc


What's about Fish & Chips?!!


Resource Descriptor Element Reference
=====================================

The following (XML) elements are defined for resource descriptors.  Some
elements are polymorous (Grammars, Cores).  See below for a reference
on the respective real elements known to the software.


Each element description gives a general introduction to the element's
use (complain if it's too technical; it's not unlikely that it is since
these texts are actually the defining classes' docstrings).

Within RDs, element properties that can (but need not) be written in XML
attributes, i.e., as a single string, are called "atomic".  Their types
are given in parentheses after the attribute name along with a default
value.

In general, items defaulted to Undefined are mandatory.  Failing to
give a value will result in an error at RD parse time.

Within RD XML documents, you can (almost always) give atomic children
either as XML attribute (``att="abc"``) or as child elements
(``<att>abc</abc>``).  Some of the "atomic" attributes actually contain
lists of items.  For those, you should normally write multiple child
elements (``<att>val1</att><att>val2</att>``), although sometimes it's
allowed to mash together the individual list items using a variety of
separators.

Here are some short words about the types you may encounter, together
with valid literals:

* boolean – these allow quite a number of literals; use ``True`` and
  ``False`` or ``yes`` and ``no`` and stick to your choice.
* unicode string – there may be additional syntactical limitations on
  those.  See the explanation
* integer – only decimal integer literals are allowed
* id reference – these are references to items within XML documents; all
  elements within RDs can have an ``id`` attribute, which can then be
  used as an id reference.  Additionally, you can reference elements
  in different RDs using <rd-id>#<id>.  Note that DaCHS does not support
  forward references (i.e., references to items lexically behind the
  referencing element).
* list of id references – Lists of id references.  The
  values could be mashed together with commas, but prefer multiple child
  elements.

There are also "Dict-like" attributes.  These are built from XML like::

  <d key="ab">val1</d>
  <d key="cd">val2</d>

In addition to key, other (possibly more descriptive) attributes for the
key within these mappings may also be allowed.  In special circumstances
(in particular with properties) it may be useful to add to a value::

  <property key="brokencols">ab,cd</property>
  <property key="brokencols" cumulative="True">,x</property>

will leave ``ab,cd,x`` in brokencols.

Many elements can also have "structure children".  These correspond to
compound things with attributes and possibly children of their own.
The name given at the start of each description is irrelevant to the
pure user; it's the attribute name you'd use when you have the
corresponding python objects.  For authoring XML, you use the name in
the following link; thus, the phrase "colRefs (contains Element
columnRef..." means you'd write ``<columnRef...>``.

Here are some guidelines as to the naming of the attributes:

* Attributes giving keys into dictionaries or similar (e.g., column
  names) should always be named ``key``
* Attributes giving references to some source of events or data
  should always be named ``source``, never "src" or similar
* Attributes referencing generic things should always be called
  ``ref``; of course, references to specific things like tables or
  services should indicate in their names what they are supposed to
  reference.

Also note that examples for the usage of almost everything mentioned
here can be found in in the `GAVO datacenter element reference`_.

.. _GAVO datacenter element reference: http://docs.g-vo.org/DaCHS/elemref.html

.. replaceWithResult getStructDocs(docStructure)


Active Tags
===========

The following tags are "active", which means that they do not directly
contribute to the RD parsed.  Instead they define, replay, or edit
streams of elements.

.. replaceWithResult getActiveTagDocs(docStructure)


Grammars Available
==================

The following elements are all grammar related.  All grammar elements
can occur in data descriptors.

.. replaceWithResult getGrammarDocs(docStructure)

Cores Available
===============

The following elements are related to cores.  All cores can only occur
toplevel, i.e. as direct children of resource descriptors.  Cores are
only useful with an id to make them referencable from services using
that core.

.. replaceWithResult getCoreDocs(docStructure)


Predefined Macros
=================

Macro expansions in DaCHS start with a backslash, arguments are given
in curly braces.  What macros are available depends on the element doing
the expansion; regrettably, not all strings are expanded, and at this
point it's not usually documented which are and which are not (though we
hope DaCHS typically behaves "as expected").  If this bites you,
complain to the authors and we promise we'll give fixing this a higher
priority.

.. replaceWithResult formatKnownMacros(docStructure)

Mixins
======

Mixins ensure a certain functionality on a table.  Typically, this is
used to provide certain guaranteed fields to particular cores.  For many
mixins, there are predefined procedures (both rowmaker applys and
grammar rowfilters) that should be used in grammars and/or rowmakers
feeding the tables mixing in a given mixin.

.. replaceWithResult getMixinDocs(docStructure, [
  "//products#table", "//scs#positions", "//scs#q3cindex",
  "//siap#bbox", "//siap#pgs", "//ssap#hcd", "//obscore#publish",
  "//obscore#publishSIAP", "//slap#basic",
  "//ssap#sdm-instance", "//ssap#mixc",
  "//obscore#publishSSAPHCD", "//epntap#table"])


Triggers
========

In the context of the GAVO DC, triggers are conditions on rows -- either
the raw rows emitted by grammars if they are used within grammars, or
the rows about to be shipped to a table if they are used within
tables.  Triggers may be used recursively, i.e., triggers may contain
more triggers.  Child triggers are normally or-ed together.

Currently, there is one useful top-level trigger, the `element
ignoreOn`_.  If an ignoreOn is triggered, the respective row is silently
dropped (actually, you ignoreOn has a bail attribute that allows you to
raise an error if the trigger is pulled; this is mainly for debugging).

The following triggers are defined:

.. replaceWithResult getTriggerDocs(docStructure)


Renderers Available
===================

The following renderers are available for allowing and URL creation.
The parameter style is relevant when adapting `condDescs`` or table
based cores to renderers:

* With clear, parameters are just handed through
* With form, suitable parameters are turned into vizier-like expressions
* With pql, suitable parameters are turned into their PQL counterparts,
  letting you specify ranges and such.

Unchecked renderers can be applied to any service and need not be
explicitely allowed by the service.

.. replaceWithResult getRendererDocs(docStructure)


Predefined Procedures
=====================

Procedures available for rowmaker apply
'''''''''''''''''''''''''''''''''''''''

.. replaceWithResult _makeProcsDocumenter([
    "//procs#simpleSelect", "//procs#resolveObject",
    "//procs#mapValue", "//procs#fullQuery", "//procs#dictMap", 
    "//siap#computePGS",
    "//siap#computeBbox", "//siap#setMeta", "//ssap#setMeta",
    "//siap#getBandFromFilter", "//ssap#setMixcMeta",
    "//slap#fillBasic",
    "//epntap#populate",
    ])(docStructure)


Procedures available for grammar rowfilters
'''''''''''''''''''''''''''''''''''''''''''

.. replaceWithResult _makeProcsDocumenter([
    "//procs#expandComma", "//procs#expandDates",
    "//products#define", "//procs#expandIntegers",
    ])(docStructure)


Procedures available for datalink cores
'''''''''''''''''''''''''''''''''''''''

.. replaceWithResult _makeProcsDocumenter(["//soda#"+ s for s in 
    "fromStandardPubDID", 
    "trivialFormatter",
    "generateProduct",
    "sdm_genDesc",
    "sdm_genData",
    "fits_genDesc",
    "fits_makeWCSParams",
    "fits_makeHDUList",
    "fits_doWCSCutout",
    "fits_formatHDUs",
    "fits_makeBANDSlice",
    "fits_makeBANDMeta",
    ])(docStructure)


Predefined Streams
==================

Streams are recorded RD elements that can be replayed into resource
descriptors using the ``FEED`` active tag.  They do, however, support
macro expansion; if macros are expanded, you need to given them values
in the FEED element (as attributes).  What attributes are required
should be mentioned in the following descriptions for those predefined
streams within DaCHS that are intended for developer consumption.

Datalink-related Streams
''''''''''''''''''''''''

.. replaceWithResult getStreamsDoc(
    ['//soda#'+id for id in ["sdm_plainfluxcalib", "sdm_cutout",
      "sdm_format", "fits_genKindPar", "fits_genPixelPar",
      "fits_standardDLFuncs", "fits_standardBANDCutout"]])

Other Streams
'''''''''''''

.. replaceWithResult getStreamsDoc(
    ['//obscore#obscore-columns']
    +['//ssap#'+id for id in ["hcd_condDescs", "atomicCoords"]]
    +['//echelle#'+id for id in ["ssacols"]]
    +['//scs#'+id for id in ["coreDescs", ]])


Metadata
========

Various elements support the setting of metadata through meta elements.
Metadata is used for conveying RMI-style metadata used in the VO
registry.  See [RMI]_ for an overview of those.  We use the keys given
in RMI, but there are some extensions discussed in `RMI-style
Metadata`_.

The other big use of meta information is for feeding templates.  Those
"local" keys should all start with an underscore.  You are basically
free to use those as you like and fetch them from your custom templates.
The predefined templates already have some meta items built in,
discussed in `Template Metadata`.

So, metadata is a key-value mapping.  Keys may be compound like in RMI,
i.e., they may consist of period-separated atoms, like
publisher.address.email.  There may be multiple items for each meta
key.

Meta inheritance
''''''''''''''''

When you query an element for metadata, it first sees if it has this
metadata.  If that is not the case, it will ask its meta parent.  This
usually is the embedding element.  It wil again delegate the request to
its parent, if it exists.  If there is no parent, configured defaults
are examined.  These are taken from rootDir/etc/defaultmeta, where they
are given as colon-separated key-value pairs, e.g.,

::

  publisher: The GAVO DC team
  publisherID: ivo://org.gavo.dc
  contact.name: GAVO Data Center Team
  contact.address: Moenchhofstrasse 12-14, D-69120 Heidelberg
  contact.email: gavo@ari.uni-heidelberg.de
  contact.telephone: ++49 6221 54 1837
  creator.name: GAVO Data Center
  creator.logo: http://vo.ari.uni-heidelberg.de/docs/GavoTiny.png

The effect is that you can give global titles, descriptions, etc.
in the RD but override them in services, tables, etc.  The configured
defaults let you specify meta items that are probably constant for
everything in your data center, though of course you can override these
in your RD elements, too.

In HTML templates, missing meta usually is not an error.  The
corresponding elements are just left empty.  In registry documents,
missing meta may be an error.

Meta formats
''''''''''''

Metadata must work in registry records as well as in HTML pages and
possibly in other places.  Thus, it should ideally be given in formats
that can be sensibly transformed into the various formats.

The GAVO DC software knows four input formats:

literal
  The textual content of the element will not be touched.  In
  HTML, it will end up in a div block of class literalmeta.

plain
  The textual content of the element will be whitespace-normalized,
  i.e., whitespace will be stripped from the start and the end,
  runs of blanks and tabs are replaced by a single blank, and empty
  lines translate into paragraphs.  In HTML, these blocks com in
  plainmeta div elements.  

rst
  The textual content of the element is interpreted as ReStructuredText_.
  When requested as plain text, the ReStructuredText itself is
  returned, in HTML, the standard docutils rendering is returned.

raw
  The textual content of the element is not touched.  It will be
  embedded into HTML directly.  You can use this, probably together
  with CDATA sections, to embed HTML -- the other formats should not
  contain anything special to HTML (i.e., they should be PCDATA in
  XML lingo).  While the software does not enforce this, raw content
  should not be used with RMI-type metadata.  Only use it for items that
  will not be rendered outside of HTML templates.


Macros in Meta Elements
'''''''''''''''''''''''

Macros will be expanded in meta items using the embedding element as
macro processors (i.e., you can use the macros defined by this element).


Typed Meta Elements
'''''''''''''''''''

While generally the DC software does not care what you put into meta
items and views them all as strings, certain keys are treated
specially.  The following meta keys trigger some special behaviour:

.. replaceWithResult getMetaKeyDocs()

Additionally, there is ``creator``, which is really special (at least
for now).  When you set creator to a string, the string will be split at
semicolons, and for each substring a creator item with the respective
name is generated.  This may sound complicated but really does about
what your would expect when you write::

  <meta name="creator">Last, J; First, B; Middle, I.</meta>


Metadata in Standard Renderers
''''''''''''''''''''''''''''''

Certain meta keys have a data center-internal interpretation, used
in renderers or writers of certain formats.  These keys should always
start with an underscore.  Among those are:

* _intro -- used by the standard HTML template for explanatory text
  above the seach form.
* _bottominfo -- used by the standard HTML template for explanatory text
  below the seach form.
* _copyright -- used by the standard HTML template for copyright-related
  information (there's also copyright in RMI; the one with the
  underscore is intended to be less formal).
* _related -- used in the standard HTML template for links to related
  services.  As listed above, this is a link, i.e., you can give a
  title attribute.
* _longdoc -- used by the service info renderer for an explanatory
  piece of text of arbitrary length.  This will usually be in
  ReStructuredText_, and we recommend having the whole meta body in a
  CDATA section.
* _news -- news on the service.  See above at `Typed Meta Elements`_.
* _warning -- used by both the VOTable and the HTML table renderer.
  The content is rendered as some kind of warning.  Unfortunately,
  there is no standard how to do this in VOTables.  There is no
  telling if the info elements generated will show anywhere.
* _noresultwarning -- displayed by the default response template instead
  of an empty table (use it for things like "No Foobar data for your
  query")
* _type -- on Data instances, used by the VOTable writer to set the
  ``type`` attribute on ``RESOURCE`` elements (to either "results"
  or "meta").  Probably only useful internally.
* _plotOptions – typically set on services, this lets you configure
  the initial appearance of the javascript-based quick plot.  The value
  must be a javascript dictionary literal (like ``{"xselIndex": 2}``)
  unless you're trying CSS deviltry (which you could, using this meta;
  then again, if you can inject RDs, you probably don't need CSS attacks).
  Keys evaluated include:

  * xselIndex – 0-based index of the column plotted on the x-axis 
    (default: 0)
  * yselIndex – 0-based index of the column plotted on the y-axis
    (default: length of the column list; that's "histogram on y)
  * usingIndex – 0-based index of the plotting style selector.  For
    now, that's 0 for points and 1 for lines.


RMI-Style Metadata
''''''''''''''''''

For services (and other things) that are registred in the Registry, you
must give certain metadata items (and you can give more), where we take
their keys from [RMI]_.  We provide a `explanatory leaflet
<./data_checklist.pdf>`_ for data providers.  The most common keys --
used by the registry interface and in part by HTML and VOTable
renderers -- include:

* title -- this should in general be given seperately on the resource,
  each table, and each service.  In simple cases, though, you may get by
  by just having one global title on the resource and rely on metdata
  inheritance.
* shortName -- a string that should indicate what the service is in 16
  characters or less.
* creationDate -- Use ISO format with time, UTC only, like this: 
  2007-10-04T12:00:00Z
* subject -- as noted in the explanatory leaflet, these should be taken
  from the `IVOA Vocabulary Explorer
  <http://explicator.dcs.gla.ac.uk/WebVocabularyExplorer/>`_.
* copyright -- freetext copyright notice.
* source -- bibcodes will be expanded to ADS links here.
* referenceURL -- again, a link, so you can give a title for
  presentation purposes.  If you give no referenceURL, the service's
  info page will be used.
* dateUpdated -- an ISO date.  Do not set this.  This is determined
  from timestamps in DaCHS's state directory.  There is also
  datetimeUpdated that you would have to keep in sync with dateUpdated
  if you were to change it.
* creator.name -- this should be the name of the "author" of the data
  set.  See below for multiple creators.  If you set this, you may want
  to override creator.logo as well.
* type – one of Other, Archive, Bibliography, Catalog, 
  Journal, Library, Simulation, Survey, Transformation, Education, 
  Outreach, EPOResource, Animation, Artwork, Background, BasicData, 
  Historical, Photographic, Press, Organisation, Project, Registry –
  it's optional and we doubt its usefulness.
* facility -- no IVOA ids are supported here yet, but probably this
  should change.
* coverage -- see the special section
* service-specific metadata (for SIA, SCS, etc.) -- see the
  documentation of the respective cores.
* utype – tables (and possibly other items) can have utypes to signify
  their role in specific data models.  For tables, this utype gets
  exported to the tap_schema.
* identifier – this is the IVORN of the resource, usually generated
  by DaCHS.  Do not override this unless you know what you are doing
  (which at least means you know how to make DaCHS declare an authority
  and claim it).  If you do override the identifier of a service that's
  already published, make sure you run 
  ``gavo admin makeDeletedRecord <previous identifier>`` (before or
  after the ``gavo pub`` on the resource, or the registries will have
  two copies of your record, one of which will not be updated any more;
  and that would suck for Registry users.

While you can set any of these in etc/defaultmeta.txt, the following items
are usually set there:

* publisher
* publisherID
* contact.name
* contact.address
* contact.email
* contact.telephone

The creator.name meta illustrates a pitfall with our metadata
definition.  Suppose you had more than one creator.  What you'd want is
a metadata structure like this::

  +-- creator -- name (Arthur)
  |
  +-- creator -- name (Berta)

However, if you write::

  creator.name: Arthur
  creator.name: Berta

or, equivalently::

  <meta name="creator.name">Arthur</meta>
  <meta name="creator.name">Berta</meta>

by the above rules, you'll get this::

  +-- creator -- name (Arthur)
         |
         +------ name (Berta)

i.e., one creator with two names.

To avoid this, make a new creator node in between, i.e., write::

  creator.name: Arthur
  creator:
  creator.name: Berta

In DaCHS resources, it's better to be explicit about the tree structure
(though you could write it like in metastream)::

  <meta name="creator">
    <meta name="name">Arthur</meta>
  </meta>
  <meta name="creator">
   <meta name="name">Berta</meta>
  </meta>

However, for creator.name specifically, it's highly likely that people
accept things like "Arthur; Berta" anyway, and so here it might be
better to disregard the tree structure issues entirely.

Actually, the DaCHS internal author table as used by the alternative
portal interprets one special notation::

  <author1>, <inits1> {; <authorn>, <initsn>}

That is, you should write authors lists like "Foo, X.; Bar, Q.; et al".


Coverage Metadata
'''''''''''''''''

Coverage metadata probably is the most complex piece of metadata, but
also potentially the most useful, since it would allow clients to
restrict querying to services known to contain relevant material.  So,
try to get it right.

Within DaCHS, coverage metadata uses the following keys:

* coverage.profile – an STC-S string giving the coverage of the service.
  These can become rather complex.  We implement several extensions to
  STC-S.  See also the `documentation for GAVO STC`_
* coverage.waveband – One of Radio, Millimeter, Infrared, Optical, UV,
  EUV, X-ray, Gamma-ray, and you can have multiple waveband
  specifications.  Note that you can provide much more detailed
  information on the covered spectral range as part of coverage.profile
  (but it's also much less likely that there is proper support for data
  there in registries and clients).
* coverage.regionOfRegard – in essence, the "pixel size" of the service in
  degrees.  If, for example, your service gives data on a lattice of
  sampling points, the typical distance of such points should be given
  here.  Leave out if this doesn't apply to your service.
* coverage.footprint – reserved; this will probably be filled in
  automatically by the software once we have a footprint standard and
  DaCHS implements it.

Here's an example for a service covering the large and small magellanic
clouds::

  <meta name="coverage">
    <meta name="profile">
      Union ICRS (
        Box 81 69.75 14 3.25
        Box 13 -73 9 2)</meta>
    <meta name="waveband">Optical</meta>
    <meta name="waveband">Infrared</meta>
    <meta name="regionOfRegard">0.02</meta>
  </meta>


.. _Documentation for GAVO STC: http://docs.g-vo.org/DaCHS/stc.html

Meta Stream Format
''''''''''''''''''

In serveral places, most notably in the ``defaultmeta.txt`` file and in
meta elements without a ``name`` attribute, you can give metadata as a
"meta stream".  This is just a sequence of lines containing pairs of
<meta key> and <meta value>.

In addition, there are comments, empty lines, and continuations.
Continuation lines work by ending a line with a backslash.  The
following line separator and all blanks and tabs following it are
then ignored.  Thus, the following two meta keys end up having identical
values::

  meta1: A contin\
    uation line needs \
      a blank if you wan\
  t one.
  meta2: A continuation line needs a blank if you want one

Note that whitespace behind a backslash prevents it from being a
continuation character.  That is, admittedly, a bit of a trap.

Other than their use as continuation characters, backslashes have no
special meaning within meta streams as such.  Within meta elements,
however, macros are expanded after continuation line processing if the
meta parent knows how to expand macros.  This lets you write things
like::

  <meta>
    creationDate: \metaString{authority.creationDate}
    managingOrg:ivo://\getConfig{ivoa}{authority}
  </meta>


Comments and empty lines are easy: Empty lines are allowed, and a
comment is a line with a hash (#) as its first non-whitespace
character.  Both constructs are ignored, and you can even continue
comments (though you should not).

Meta information can have a complex tree structure.  With meta streams,
you can build trees by referencing dotted meta identifiers.  If you
specify meta information for an item that already exists, a sibling will
be created.  Thus, after::

  creator.name: A. Author
  creator:
  creator.name: B. Buthor

there are two creator elements, each specifying a name meta.  For the
way creators are specified within VOResource, the following would be
wrong::

  creator.name: This is wrong.
  creator.name: and will not work

-- you would have a single creator meta with two name metas, which is
not allowed by VOResource.

If you write::

  contact.address: 7 Miner's Way, Behind the Seven Mountains
  contact.email: dwarfs@fairytale.fa

you have a single contact meta giving address and email.


Display Hints
=============

Display hints use an open vocabulary.  As you add value formatters, you can 
evaluate any display hint you like.  Display hints understood by the
built-in value formatters include:

checkmark
  in HTML tables, render this column as empty or checkmark depending on
  whether the value is false or true to python.

displayUnit
  use the value of this hint as the unit to display a value in.

nopreview
  if this key is present with any value, no HTML code to generate
  previews when mousing over a link will be generated.

sepChar
  a separation character for sexagesimal displays and the like.

sf
  "Significant figures" -- length of the mantissa for this column.
  Will probably be replaced by a column attribute analoguous to what
  VOTable does.

type
  a key that gives hints what to do with the column.  Values currently
  understood include:

  bar
    display a numeric value as a bar of length value pixels.

  bibcode
    display the value as a link to an ADS bibcode query.

  humanDate
    display a timestamp value or a real number in either yr (julian
    year), d (JD, or MJD if xtype is mjd), or s (unix timestamp) as 
    an ISO string.

  humanDay
    display a timestamp or date value as an ISO string without time.

  humanTime
    display values as h:m:s.

  keephtml
    lets you include raw HTML.  In VOTables, tags are removed.

  product
    treats the value as a product key and expands it to a URL for the
    product (i.e., typically image).  This is defined in
    protocols.products.  This display hint is also used by, e.g., the tar
    format to identify which columns should contribute to the tar file.

  dms
    format a float as degree, minutes, seconds.

  simbadlink
    formats a column consisting of alpha and delta as a link to query
    simbad.  You can add a coneMins displayHint to specify the search
    radius.

  suppress
    do not automatically include this column in any table (e.g.,
    verbLevel-based column selection).

  hms
    force formatting of this column as a time (usually for RA).

  url
    makes value a link in HTML tables.  The anchor text will be the last
    element of the path part of the URL, or, if given, the value of the
    anchorText property of the column (which is for cases when you want
    a constant text like "Details").  If you need more control over the
    anchor text, use an outputField with a formatter.

  imageURL
    makes value the src of an image.  Add width to force a certain
    image size.

noxml
  if 'true' (exactly like this), do not include this column in VOTables.


Note that not any combination of display hints is correctly
interpreted.  The interpretation is greedy, and only one formatter at a
time attempts to interpret display hints.


Building Service Interfaces
===========================

Within DaCHS, an HTTP request is processed as follows:

1) The core is adapted to the renderer; this means that condDescs with
   buildFrom are converted to inputKeys according to the rules of the
   renderer.  The form renderer generates VizieR-like expressions,
   protocol renderers make PQL parameters, etc.  Also, onlyForRenderer
   and notForRenderer inputKeys are selected or deselected
2) From the core's inputTable, the service builds an input data
   descriptor (unless the service has an inputDD defined already, of
   course).  Most standard cores only take input from an input table's
   parameters (the exception being the computedCore), and hence the
   automatic inputDD will only have a parmaker.  The automatic
   inputDD will parse using ContextGrammar without a rowKey (i.e.,
   no rows will be produced).  The parmaker within the automatic inputDD
   parses the input with the default parsers and using the getHTTPPar
   rowmaker function.
3) The service will build the input table using its inputDD.  The input
   must be like nevow request.args, mapping each key to a sequence of
   strings.
4) The input table is passed to the core, which produces either a table,
   a data instance, or a pair of mime-type and content.
5) From the core result, an SvcResult is built.  This is relevant when
   the service has an outputTable defined, in which case the table
   structure is adapted if the input actually is a table.
6) The renderer formats the SvcResult according to its wishes.

There is special handling for the form renderer, which does its parsing
using nevow formal.  For it, the input table is built by just putting
the values of the dictionary nevow formal produces into the input table
params.


TBD: multiplicity, param values as defaults,


Table-based cores
'''''''''''''''''

You will usually deal with cores querying database tables – dbCore,
ssapCore, etc.  For these, there should not be a need to define an
inputDD; the one generated from the condDescs should work fine.

To create simple constraints, just ``buildFrom`` the columns queried::

  <condDesc buildFrom="myColumn"/>

(the names are resolved in the core's queried table).  This pattern has
the advantage that the concrete parameter style is adapted to the
renderer – in the web interface, there are vizier-like expressions, in
protocol interfaces, you get fields understanding expressions as in
SSAP's "PQL", plus in addition "structured parameters" (like FOO_MIN and
FOO_MAX) where applicable.

This will generate query fields that work against data as stored in the
database, with some exceptions (columns containing MJDs will, for
example, be turned into VizieR-like date expressions for web forms).
For protocol input, this is, in general, what you want.  In web forms,
you may want to customize the apprearance, for example, to adapt to
user's unit preferences.  For this latter use case, there is the
``inputUnit`` attribute::

    <condDesc>
      <inputKey original="minDist" inputUnit="arcsec"
        type="vexpr-float">
        <property key="onlyForRenderer">form</property>
      </inputKey>
      <inputKey original="minDist" 
        type="pql-float">
        <property key="notForRenderer">form</property>
      </inputKey>
    </condDesc>

Note how in this case we adapted the types of the input keys to provide
interfaces suitable to the various renderers.  For HTML forms, we
recommend one of

* vexpr-float
* vexpr-date (dates with timestamps in the database)
* vexpr-mjd (dates with MJD in the database)
* vexpr-string (though for those, frequently generating options is
  preferable, see below)

For protocol input, the types available are

* pql-int
* pql-float
* pql-string
* pql-date (where timestamps are in the database; MHD works fine with
  pql-float since date input is not really desirable for protocol input
  anyway).

Note that you can, of course, also keep the default types where that
provides a better interface.  Flag-like integers, for example, are
classic examples where giving the possible values is preferable to
allowing parameter expressions.

For object lists and similar, it is frequently desirable to give the
possible values (unless there are too many of those; these will be
translated to option lists in forms and to metadata items for protocol
services and hence be user visible)::

   <condDesc>
      <inputKey original="source">
        <values fromdb="source from plc.data"/>
      </inputKey>
    </condDesc>


All these generate the default SQL, which is equality (or set membership
for multiple values for a parameter).  To generate custom SQL, give a
phraseMaker, like this::

   <condDesc>
      <inputKey original="confirmed" multiplicity="single"/>
      <phraseMaker>
        <code>
          if inPars.get(inputKeys[0].name, False):
            yield "confirmed"
        </code>
      </phraseMaker>
    </condDesc>

PhraseMakers work like other code embedded in RDs (and thus may have
setup).  ``inPars`` gives a dictionary of the input parameters as parsed
by the inputDD according to multiplicity (or as delivered by nevow
formal – use the ``getHTTPPar`` rowmaker function if there can be input
with differing multiplicities).  ``inputKeys`` contains a sequence of
the condDesc's inputKeys.  By using their names as above, your code will
not break if the parameters are renamed.

PhraseMakers must yield zero or more SQL fragments; multiple SQL
fragments are joined in conjunctions (i.e., end up in ANDed conditions
in the WHERE clause).

Since you are dealing with raw SQL here, *never* include material from
inPars directly in the query strings you return – this would immediately
let people to SQL injections at least when the inputKey's type is
string.  Instead, use getSQLKey as in this example::

    <condDesc>
      <inputKey original="hdwl" multiplicity="single"/>
      <phraseMaker>
        <code>
          ik = inputKeys[0]
          destRE = "^%s\\.[0-9]*$"%inPars[ik.name]
          yield "%s ~ (%%(%s)s)"%(ik.name,
            base.getSQLKey("destRE", destRE, outPars))
        </code>
      </phraseMaker>
    </condDesc>

``getSQLKey`` takes a suggested name, a value and a dictionary, which
within phraseMakers always is ``outPars``. It will enter value with the
suggested name as key into outPars or change the suggested name if there
is a name clash.  The generated name will be returned, and that is what
is entered in the SQL statement.

The ``outPars`` dictionary is shared between all condDescs entering into
a query.  Hence, if you do anything with it except passing it to
``base.getSQLKey``, you're voiding your entire warranty.

Here's how to define a condDesc doing a full text search in a column::

  <condDesc>
    <inputKey original="source" description="Words from the catalog
      description, e.g., author names or title words."/>
    <phraseMaker>
      <code>
        yield ("to_tsvector('english', source)"
          " @@ plainto_tsquery('english', %%(%s)s)")%(
        base.getSQLKey("source", inPars["source"], outPars))
      </code>
    </phraseMaker>
  </condDesc>

Incidentally, this would go with an index definition like::

  <index columns="source" method="gin"
    >to_tsvector('english', source)</index>

For the HTML form interface, you can influence the widgets chosen by the
renderer to some extent.  To get an options list allowing multiple
selections, say::

    <condDesc>
      <inputKey original="carsfield" multipliticy="multiple">
        <values fromdb="carsfield from carsarcs.meta order by carsfield"/>
      </inputKey>
    </condDesc>

Use the ``showItems="n"`` attribute of inputKeys to determine how many
items in the selector are shown at one time.

For special effects, you can group inputKeys.  This will make them show
up under a common label and in a single line in HTML forms.  Here's an
example for a simple range selector::

  <condDesc>
    <inputKey name="el" type="text" tablehead="Element"/>

    <inputKey name="mfmin" tablehead="Min. Mass Fraction \item">
      <property name="cssClass">a_min</property>
    </inputKey>

    <inputKey name="mfmax" tablehead="Max. Mass Fraction \item">
      <property name="cssClass">a_max</property>
    </inputKey>

    <group name="mf">
      <description>Mass fraction of an element. You may leave out
        either upper or lower bound.</description>
      <property name="label">Mass Fraction between...</property>
      <property name="style">compact</property>
    </group>
  </condDesc>

You will probably want to style the result of this effort using the
``service`` element's ``customCSS`` property, maybe like this::

  <service...>
    <property name="customCSS">
      input.a_min {width: 5em}
      input.a_max {width: 5em}
      input.formkey_min {width: 6em!important}
      input.formkey_max {width: 6em!important}
      span.a_min:before { content:" between "; }
      span.a_max:before { content:" and "; }
      tr.mflegend td {
        padding-top: 0.5ex;
        padding-bottom: 0.5ex;
        border-bottom: 1px solid black;
      }
    </property>
  </service>

See also the entries on `multi-line input`_, `selecting input fields
with a widget`_, and `customizing generated SCS conditions`_.

.. _multi-line input: howDoI.html#get-a-multi-line-text-input-for-an-input-key
.. _selecting input fields with a widget: howDoI.html#make-an-input-widget-to-select-which-columns-appear-in-the-output-table
.. _customizing generated SCS conditions: howDoI.html#change-the-query-issued-on-scs-queries

TBD: Say something about required.  Do we even want to mention
widgetFactory?

Formatting the output
'''''''''''''''''''''

TBD


Regression Testing
==================

Introduction
''''''''''''

Things break – perhaps because someone foolishly dropped a database
table, because something happened in your upstream, because you changed
something or even because we changed the API (if that's not mentioned in
Changes, we owe you a beverage of your choice).  Given that, having
regression tests that you can easily run will really help your peace of
mind.

Therefore, DaCHS contains a framework for embedding regression tests in
resource descriptors.  Before we tell you how these work, some words of
advice, as writing useful regression tests is an art as much as
engineering.

*Don't overdo it.*  There's little point in checking all kinds of
functionality that only uses DaCHS code – we're running our tests before
committing into the repository, and of course before making a release.
If the services just use condDescs with buildFrom and one of the
standard renderers, there's little point in testing beyond a request
that tells you the database table is still there and contains something
resembling the data that should be there.

*Don't be over-confident.*  Just because it seems trivial doesn't
mean it cannot fail.  Whatever code there is in the service processing
of your RD, be it phrase makers, output field formatters, custom
render or data functions, not to mention custom renderers and cores,
deserves regression testing.

*Be specific.*  In choosing the queries you test against, try to find
something that won't change when data is added to your service, when
you add input keys or when doing similar maintenance-like this.  Change
will happen, and it's annoying to have to fix the regression test every
time the output might legitimately change.  This helps with the next point.

*Be pedantic.*  Do not accept failing regression tests, even if you
think you know why they're failing.  The real trick with useful testing
is to keep "normal" output minimal.  If you have to "manually" ignore
diagnostics, you're doing it wrong.  Also, sometimes tests may fail
"just once".  That's usually a sign of a race condition, and you should
*really* try to figure out what's going on.

*Make it fail first.*  It's surprisingly easy to write no-op tests
that run but won't fail when the assertion you think you're making is no
longer true.  So, when developing a test, assert something wrong first,
make sure there's some diagnostics, and only then assert what you really
expect.

*Be terse.*  While in unit tests it's good to test for maximally
specific properties so failing unit tests lead you on the right track as
fast as possible, in regression tests there's nothing wrong with
plastering a number of assertions into one test.  Regression tests
actually make requests to a web server, and these are comparatively
expensive.  The important thing here is that regression testing is fast
enough to let you run them every time you make a change.


Writing Regression Tests
''''''''''''''''''''''''

DaCHS' regression testing framework is organized a bit along the lines
of python's unittest and its predecessors, with some differences due to
the different scope.

So, tests are grouped into suites, where each suite is contained in a
regSuite_ element.  These have a (currently unused) title and a boolean
attribute ``sequential`` intended for when the tests contained must be
executed in the sequence specified and not in parallel.  It defaults to
false, which means the requests are made in random order and in
parallel, which speeds up the test runs and, in particular, will help
uncover race conditions.

On the other hand, if you're testing some sort of interaction across
requests (e.g., make an upload, see if it's there, remove it again),
this wouldn't work, and you must set `sequential="True"`.  Keep these
sequential suites as short as possible.  In tests within such suites
(and only there), you can pass information from one test to the
following one by adding attributes to ``self.followUp`` (which are
available as attributes of self in the next test).   If you need to
manipulate the next URL, it's at ``self.followUp.url.content_``.  For the
common case of a redirect to the url in the location header (or a child
thereof), there's the ``pointNextToLocation(child="")`` method of
regression tests.  In the tests that are manipulated like this, the URL
given in the RD should conventionally be ``overridden in the previous
test``.  Of course, additional parameters, httpMethods, etc, are still
applied in the manipulated url element.

Regression suites contain tests, represented in regTest_ elements.
These are procDefs (just like, e.g., rowmakery ``apply``), so you can
have setup code, and you could have a library of parametrizable regTests
procDefs that you'd then turn into regTests by setting their parameters.
We've not found that terribly useful so far, though.

You must given them a ``title``, which is used when reporting problems
with them.  Otherwise, the crucial children of these are ``url`` and, as
always with procDefs, ``code``.

Here are some hints on development:

1) Give the test you're just developing an id; at the GAVO DC, we're
   usually using cur; that way, we run variations of 
   ``gavo test rdId#cur``, and only the test in question is run.
2) After defining the url, just put an ``assert False`` into the test
   code.  Then run ``gavo test -Devidence.xml rdId#cur`` or similar.
   Then investigate ``evidence.xml`` (possibly after piping through
   ``xmlstarlet fo``) for stable and strong indicators that things are
   working.
3) If you get a BadCode for a test you're just writing, the message may
   not always be terribly helpful.  To see what's actually bugging
   python, run ``gavo --debug test ...`` and check dcInfos.

RegTest URLs
''''''''''''

The `url element`_ encapsulates all aspects of building the request.  In
the simplest case, you just can have a simple URL, in which case it
works as an attribute, like this::
  
  <regTest title="example" url="svc/form">
    ...

URLs without a scheme and a leading slash are interpreted relative to
the RD's root URL, so you'd usually just give the service id and the
renderer to be applied.  You can also specify root-relative and fully
specified URLs as described in the documentation of the `url element`_. 

White space in URLs is removed, which lets you break long URLs as
convenient.

You could have GET parameters in this URL, but that's inconvient due to
both XML and HTTP escaping.  So, if you want to pass parameters, just
give them as attributes to the element::

  <regTest title="example">
    <url RA="10" DEC="-42.3" SR="1" parSet="form">svc/form</url>

The ``parSet=form`` here sets up things such that processing for the
form renderer is performed – our form library nevow formal has some
hidden parameters that you don't want to repeat in every URL.  

To easily translate URLs taken from a browser's address bar or the form
renderer's result link, you can run ``gavo totesturl`` and paste the
URLs there.  Note that totesturl fails for values with embedded quotes,
takes only the first value of repeated parameters and is a over-quick
hack all around.  Patches are gratefully accepted.

The ``url`` element hence accepts arbitary attributes, which can be a
trap if you think you've given values to url's private attributes and
mistyped their names.  If uploads or authentication don't seem to
happen, check if your attribute ended up the in the URL (which is
displayed with the failure message) and fix the attribute name; 
most private url attributes start with ``http``.  If you really need
to pass a parameter named like one of url's private attributes, pass it
in the URL if you can.  If you can't because you're posting, spank us.
After that, we'll work out something not too abominable .

If you have services requiring authentication, use url's ``httpAuthKey``
attribute.  We've introduced this to avoid having credentials in the RD,
which, after all, should reside in a version control system which may
be (and in the case of GAVO's data center is) public.  The attribute's
value is a key into the file ``~/.gavo/test.creds``, which contains, line
by line, this key, a username and a password, e.g.::

  svc1 testuser notASecret
  svc2 regtest NotASecretEither

A test using this would look like this::

  <regTest title="Authenticated user can see the light">
    <url httpAuthKey="svc1">svc1/qp/light.txt</url>
    <code>
      self.assertHTTPStatus(200)
    </code>
  </regTest>

By default, a test will perform a GET request.  To change this, set
the ``httpMethod`` attribute.  That's particularly important with
uploads (which must be POSTed).  

For uploads, the url element offers two facilities.  You can set a
request payload from a file using the ``postPayload`` attribute (the
path is interpreted relative to the resource directory), but it's much
more common to do a file upload like browsers do them.  Use the
``httpUpload`` element for this, as in::
  
  <url> <httpUpload name="UPLOAD"
    fileName="remote.txt">a,b,c</httpUpload> svc1/async </url>

(which will work as if the user had selected a file remote.txt
containing "a,b,c" in a browser with a file element named UPLOAD), or as
in::

  <url>
    <httpUpload name="UPLOAD" fileName="remote.vot"
      source="res/sample.regtest"/>
    svc1/async
  </url>

(which will upload the file referenced in ``source``, giving the remote
server the filename ``remote.vot``).  The ``fileName`` attribute is
optional.

Finally, you can pass arbitrary HTTP headers using the ``httpHeader``
element.  This has an attribute ``key``; the header's value is taken
from the element content, like this::

  <url postPayload="res/testData.regtest" httpMethod="POST">
    <httpHeader key="content-type">image/jpeg</httpHeader>
    >upload/custom</url>

RegTest Tests
'''''''''''''

Since regression tests are just procDefs, the actual assertions are
contained in the ``code`` child of the ``regTest``.  The code in there
sees the test itself in self, and it can access ``self.data`` (the
response content), ``self.headers`` (a sequence of header name, value
pairs; note that you should match the names case-insensitively here),
and ``self.status`` (the HTTP response code), as well as the URL
actually retrieved in ``self.url.httpURL`` (incidentally, that name is
right; the regression framework only supports http, and it's not
terribly likely that we'll change that).

You should probably only access those attributes in a pinch and instead
use the pre-defined assertions, which are methods on the test objects as
in pyunit – conventional assertions are clearer to read and less likely
to break if fixes to the regression test API become necessary.  If you
still want to have custom tests, raise AssertionErrors to indicate a
failure.

Here's a list of assertion methods defined right now:

.. replaceWithResult getRegtestAssertions(docStructure)


All of these are methods, so you would actually write
``self.assertHasStrings('a', 'b', 'c')`` in your test code (rather than
pass self explicitely.

When writing tests, you can, in addition, use assertions from python's
unittest TestCases (e.g., assertEqual and friends).  This is provided in
particular for use to check values in VOTables coming back from services
together with the ``getFirstVOTableRow`` method.

When writing tests, please note that, like all procDef's bodies, the test
code is macro-expanded by DaCHS.  This means that every backslash that
should be seen by python needs to be escaped itself (i.e., doubled).  An
escaped backslash in python thus is four backslashes in the RD.

Finally, here's a piece of ``.vimrc`` that inserts a ``regTest``
skeleton if you type ge in command mode (preferably at the start of
a line; you may need to fix the indentation if you're not indenting with
tabs.  We've thrown in a column skeleton on gn as well::

  augroup rd
    au!
    autocmd BufRead,BufNewFile *.rd set ts=2 tw=79
    au BufNewFile,BufRead *.rd map gn i<tab><tab><lt>column name="" type=""<CR><tab>unit="" ucd=""<CR>tablehead=""<CR>description=""<CR>verbLevel=""/><CR><ESC>5kf"a
    au BufNewFile,BufRead *.rd map ge i<tab><tab><lt>regTest title=""><CR><tab><lt>url><lt>/url><CR><lt>code><CR><lt>/code><CR><BS><lt>/regTest><ESC>4k
  augroup END


Running Tests
'''''''''''''

The first mode to run the regression tests is through ``gavo val``.  If
you give it a ``-t`` flag, it will collect regression tests from all the
RDs it touches and run them.  It will then output a brief report listing
the RDs that had failed tests for closer inspection.

It is recommended to run something like::
  
  gavo val -tv ALL

before committing changes into your inputs repository.  That way,
regressions should be caught.

The tests are ran against the server described through the
``[web]serverURL`` config item.  In the recommended setup, this would be
a server started on your own development machine, which then would
actually test the changes you made.

There is also a dedicated gavo sub-command ``test`` for executing the
tests.  This is what you should be using for developing tests or
investigating failures flagged with ``gavo val``.  On its command line,
you can give on of an RD id or a cross-rd reference to a test suite,
or a cross-rd reference to an individual test.  For example,

::

  gavo test res1/q 
  gavo test res2/q#suite1 
  gavo test res2/q#test45

would run all the tests given in the RD ``res1/q``, the tests in
the regSuite with the ``id`` suite1 in ``res2/q``, and a test with
``id="test45`` in ``res2/q``, respectively.

To traverse inputs and run tests from all RDs found there, as well as
tests from the built-in RDs, run::

  gavo test ALL

``gavo test`` by default has a very terse output.  To see which tests
are failing and what they gave as reasons, run it with the '-v' option.

To debug failing regression tests (or maybe to come up with good things
to test for), use '-d', which dumps the server response of failing tests
to stdout.

In the recommended setup with a production server and a development
machine sharing a checkout of the same inputs, you can exercise
production server from the development machine by giving the ``-u``
option with what your production server has in its ``[web]serverURL``
configuration item.  So,

::

  gavo test -u http://production.example.com ALL

is what might help your night's sleep.


Examples
''''''''

Here are some examples how these constructs can be used.  First, a
simple test for string presence (which is often preferred even when
checking XML, as it's less likely to break on schema changes; these
usually count as noise in regression testing).  Also note how we have
escaped embedded XML fragments; an alternative to this shown below
is making the code a CDATA section::

  <regTest title="Info page looks ok" 
    url="siap/info">
    <code>
      self.assertHasStrings("SIAP Query", "siap.xml", "form", 
        "Other services", "SIZE&lt;/td>", "Verb. Level")
    </code>
  </regTest>

The next is a test with a "rooted" URL that's spanning lines, has
embedded parameters (not recommended), plus an assertion on binary
data::

  <regTest title="NV Maidanak product delivery"
    url="/getproduct/maidanak/data/Q2237p0305/Johnson_R/
      red_kk050001.fits.gz?siap=true">
    <code>
      self.assertHasStrings('\\x1f\\x8b\\x08\\x08')
    </code>
  </regTest>

This is how parameters should be passed into the request::

  <regTest title="NV Maidanak SIAP returns accref.">
    <url POS="340.12,3.3586" SIZE="0.1" INTERSECT="OVERLAPS" 
      _TDENC="True" _DBOPTIONS_LIMIT="10">siap/siap.xml</url>
    <code>
      self.assertHasStrings('&lt;TD>AZT 22')
    </code>
  </regTest>

Here's an example for a test with URL parameters and xpath assertions::

  <regTest title="NV Maidanak SIAP metadata query"
      url="siap/siap.xml?FORMAT=METADATA">
    <code>
      self.assertXpath("//v1:FIELD[@name='wcs_cdmatrix']", {
        "datatype": "double",
        "ucd": "VOX:WCS_CDMatrix",
        "arraysize": "*",
        "unit": "deg/pix"})
      self.assertXpath("//v1:INFO[@name='QUERY_STATUS']", {
        "value": "OK",
        None: "OK",})
      self.assertXpath("//v1:PARAM[@name='INPUT:POS']", {
        "datatype": "char",
        "ucd": "pos.eq",
        "unit": "deg"})
    </code>
  </regTest>

The following is a fairly complex example for a stateful suite doing
inline uploads (and simple tests)::

  <regSuite title="GAVO roster publication cycle" sequential="True">
    <regTest title="Complete record yields some credible output">
      <url httpAuthKey="gvo" parSet="form" httpMethod="POST">
        <httpUpload name="inFile" fileName="testing_ignore.rd"
          ><![CDATA[
            <resource schema="gvo">
              <meta name="description">x</meta>
              <meta name="title">A test service</meta>
              <meta name="creationDate">2010-04-26T11:45:00</meta>
              <meta name="subject">Testing</meta>
              <meta name="referenceURL">http://foo.bar</meta>
              <nullCore id="null"/>
              <service id="run" core="null" allowed="external">
                <meta name="shortName">u</meta>
                <publish render="external" sets="gavo">
                  <meta name="accessURL">http://foo/bar</meta>
                </publish></service></resource>
          ]]></httpUpload>upload/form</url>
      <code><![CDATA[
        self.assertHasStrings("#Published</th><td>1</td>")
      ]]></code>
    </regTest>

    <regTest title="Publication leaves traces on GAVO list" url="list/custom">
      <code>
        self.assertHasStrings(
          '"/gvo/data/testing_ignore/run/external">A test service')
      </code>
    </regTest>

    <regTest title="Unpublication yields some credible output">
      <url httpAuthKey="gvo" parSet="form" httpMethod="POST">
        <httpUpload name="inFile" fileName="testing_ignore.rd"
          ><![CDATA[
          <resource schema="gvo">
            <meta name="description">x</meta>
            <meta name="title">A test service</meta>
            <meta name="creationDate">2010-04-26T11:45:00</meta>
            <meta name="subject">Testing</meta>
            <meta name="referenceURL">http://foo.bar</meta>
            <service id="run" allowed="external">
              <nullCore/>
              <meta name="shortName">u</meta></service></resource>
          ]]></httpUpload>upload/form</url>
      <code><![CDATA[
        self.assertHasStrings("#Published</th><td>0</td>")
      ]]></code>
    </regTest>

    <regTest title="Unpublication leaves traces on GAVO list"
      url="list/custom">
      <code>
        self.assertLacksStrings(
          '"/gvo/data/testing_ignore/run/external">A test service')
      </code>
    </regTest>

  </regSuite>

If you still run SOAP services, here's one way to test them::

  <regTest id="soaptest" title="APFS SOAP returns something reasonable">
      <url postPayload="res/soapRequest.regtest" httpMethod="POST">
        <httpHeader key="SOAPAction">'"useService"'</httpHeader>
        <httpHeader key="content-type">text/xml</httpHeader
        >qall/soap/go</url>
      <code>
        self.assertHasStrings(
          '="xsd:date">2008-02-03Z&lt;/tns:isodate>', 
        '&lt;tns:raCio xsi:type="xsd:double">25.35')
      </code>
    </regTest>

– here, ``res/soapRequest.regtest`` would contain the request body that
you could, for example, extract from a tcpdump log.

.. _regSuite: #element-regsuite
.. _regTest: #element-regtest
.. _url element: #element-url




Datalink Cores
==============

**Historical note:**   DaCHS, which actually did much of the prototyping
for the respective IVOA standards, treats datalink (metadata management)
and SODA (actual operations on data) as a fairly integral system; the
separation between the two does not make much sense within DaCHS.
During prototyping, many of the procDefs and STREAMs for data
manipulation mentioned below started their lives in the //datalink RD.
Although they have not been removed there yet, please do not take them
from there any more.  Take them from //soda, which is where they are
maintained and developed.

Datalink is an IVOA protocol that allows associating various products
and artifacts with a data set id.  Think the association of error or
mask maps, progenitor datasets, or processed data products, with a data
set.  

It also lets you associate data processing services with datasets,
which allows on-the-fly generation of cutouts, format conversions or
recalibrations; the details of how this is done are described in another
standard called SODA.  In DaCHS, Datalink is implemented by the
``dlmeta`` renderer, SODA by the ``dlget`` renderer.  In all but fairly
exotic cases, both renderers are used on the same service.  In the
following treatment, we do not really distinguish between Datalink and
SODA.

A central term for datalink is the pubDID, or publisher DID.  This is an
identifier assigned (essentially) by you that points to a concrete
dataset.  In DaCHS, datalink services always use pubDIDs as the values
of the datalink ID parameter.

Within DaCHS, you can write datalink services using a specialized type
of core.  Its function is twofold:

(1) when operated by the dlmeta renderer, it returns the access options 
    ("Datalink document")
(2) when operated by the dlget renderer, it performs some computation
    ("Processed data")

Function (1) is implemented by DaCHS code working on the metadata of the
Datalink core.  Function (2) requires custom code (or the assembly of
pre-provided building blocks).

A datalink core consists of 

* exactly one descriptor generator,
* zero or more data functions, generating and manipulating data
* zero or one formatters, formatting the generated and/or manipulated
  data
* zero or more meta makers, generating input parameter descriptions
  for data functions and any formatter present and/or related links

Here's how they work together in providing the Datalink functionality:

To generate the Datalink document, the descriptor generator is passed
the pubDID and is expected to return a ``datalink.ProductDescriptor``
instance (or None, in which case the datalink request will be rejected
by a 404).  In addition to attributes named after the columns of the
product table (and potentially other attributes added by deriving from
the base ProductDescriptor), it has an attribute ``data`` defaulting to
``None``, intended to be filled by the core's data generator on data
processing runs.

The descriptor is then passed, in turn, to the meta makers, which yield
``InputKey`` or ``LinkDef`` instances to describe the retrival options
for the product.  The combination of both is then formatted to a proper
Datalink document and returned, which concludes the processing of the
metadata request.

When a request for processed data comes in, the descriptor generator is
again used to make a product descriptor, and again the input keys are
updated as before.  They are then used to build the arguments described
by the input keys.  

If the context grammar succeeds, the data descriptor is passed to the
first data function together with the arguments parsed.  This must fill
out the ``data`` attribute of the descriptor or raise a ValidationError
for the PUBDID; leaving descriptor as ``None`` results in a 500 server
error.  Descriptor.data could an ``rsc.InMemoryTable`` (e.g., in SSAP)
or a ``products.Products`` instance, but as long as the other data
functions and the formatter agree on what it is, anything goes.  It will
usually be fed from a database, pixels in FITS files, or the like.

This object is then handed through all remaining data functions; these
change the data in place or create a new one as convenient and
manipulate ``descriptor.data`` accordingly.

Finally, the data enters the formatter, which actually generates the
output, usually returning a pair of mime type and string to be
delivered.

It is a design descision which manipulations are done in the data
generator, which are in later filters, and which perhaps only in the
formatter.  The advantage of filters is that they are more flexible and
can more easily be reused, while doing it things in the data generator
itself will usually be more efficient, sometimes much so (e.g., sums
being computed within a database rather than in a filter after all the
data had to go through the interface of the database).

Incidentally (we mention it here for lack of a better place) DaCHS
automatically adds links for the dataset itself (semantics #this) and a
preview of the dataset (semantics #this) whenever sensible.  There are
exotic situations in which that is unwelcome.  If you end up in such a
situation, in a metaMaker say ``descriptor.suppressAutoLinks = True``.

Descriptors Generators
'''''''''''''''''''''''

Descriptor generators (see `element descriptorGenerator`_) are procedure
applications that see a pubDID value and are expected to return a
``datalink.ProductDescriptor`` instance, or something derived from it.  

Simple Product Descriptor Generator
...................................

In the end, this usually boils down to figuring out the value of accref
in the product table and using what's there to construct the descriptor
generator.  In the simplest case, the pubDID will be in DaCHS'
"standard" format (see the ``getStandardPubDID`` rowmaker function), in
which case the default descriptor generator works and you don't have to
specify anything.  You could manually insert that default by saying::
  
  <descriptorGenerator procDef="//soda#fromStandardPubDID"/>

(this happens to be DaCHS' default if no descriptor generator is given).
It's functionality is equivalent to this::

  <descriptorGenerator>
    <code>
      return ProductDescriptor.fromAccref(pubDID,
        "/".join(pubDID.split("/")[4:]))
    </code>
  </descriptorGenerator>

– which might be a good place to start if you need to write your own
d.g., e.g., because you have some special logic to encode the accref in
the PubDID).

Incidentally, such custom pubDID schemes are what makes this so obscure.
We'll try to come up with something a bit clearer as soon as the
Datalink dust has settled.

The default ``ProductDescriptor`` class exposes all the columns from the
products table, i.e., accref, accessPath, mime, owner, embargo, 
sourceTable, datalink (a specialised datalink service for this data
set),
preview, and previewMime in addition to the pubDID itself.

Spectrum Product Descriptor Generators
......................................

A slightly more interesting example is provided by datalink for SSA,
where cutouts and similar is generated from spectra.  The actual
definition is in ``//soda#sdm_genDesc``, but the gist of it is::

  <descriptorGenerator>
    <setup>
      <par key="ssaTD" description="Full reference (like path/rdname#id)
        to the SSA table the spectrum's PubDID can be found in."/>

      <code>
        from gavo import rsc
        from gavo import rscdef
        from gavo import svcs

        class SSADescriptor(ProductDescriptor):
          ssaRow = None

          @classmethod
          def fromSSARow(cls, ssaRow, paramDict):
            """returns a descriptor from a row in an ssa table and
            the params of that table.
            """
            paramDict.update(ssaRow)
            ssaRow = paramDict
            res = cls.fromAccref(ssaRow['accref'])
            res.ssaRow = ssaRow
            return res
      
        ssaTD = base.resolveCrossId("myres/q#mytable, rscdef.TableDef)
      </code>
    </setup>
    
    <code>
      with base.getTableConn() as conn:
        ssaTable = rsc.TableForDef(ssaTD, connection=conn)
        matchingRows = list(ssaTable.iterQuery(ssaTable.tableDef, 
          "ssa_pubdid=%(pubDID)s", {"pubDID": pubDID}))
        if not matchingRows:
          raise svcs.UnknownURI("No spectrum with pubDID %s known here"%
            pubDID)

        # the relevant metadata for all rows with the same PubDID should
        # be identical, and hence we can blindly take the first result.
        return SSADescriptor.fromSSARow(matchingRows[0],
          ssaTable.getParamDict())
    </code>
  </descriptorGenerator>

Note how we derive from ProductDescriptor to get something that metadata
makers can later consult to figure out the spectral extent, the
calibration status, etc., by combining a row from an SSA table and its
parameter dict and stuffing that into an attribute of the derived class.
Also, since SSA tables already contain a column containing PubDIDs, we
can treat them as opaque.

Incidentally, in this case you could stuff the entire code into the the
main code element, saving on the extra setup.  However, apart from a
minor speed benefit, keeping things like function or class definitions
in setup allows easier re-use of such definitions in procedure
applications and is therefore recommended.


FITS Product Descriptor Generators
..................................

For FITS files, you will usually just just //soda#fits_genDesc,
preferably giving its ``accrefStart`` parameter to prevent confusion.


Custom Product Descriptor Generators
....................................

When the ``accessPath`` in the products table is the datalink already – as
is sometimes desirable with large datasets –, the standard descriptor
generators don't work, because they pull the physical file path from the
product table.

You therefore need to override the computation of the path.  This is
done by deriving from the product descriptor path and manipulating its
arguments.  Use the following pattern; this uses the
``FITSProductDescriptor``, as its base class, but the principle otherwise
is the same::

  <descriptorGenerator procDef="//soda#fits_genDesc" 
      name="genFITSDesc">
    <bind key="accrefPrefix">'califa/data'</bind>
    <setup>
      <code>
        class CALIFADescriptor(FITSProductDescriptor):
          def __init__(self, *args, **kwargs):
            kwargs["accessPath"] = os.path.join(
              base.getConfig("inputsDir"),
              kwargs["accref"])
            FITSProductDescriptor.__init__(self, *args, **kwargs)
      </code>
    </setup>
    <code>
      return getFITSDescriptor(pubDID, CALIFADescriptor)
    </code>
  </descriptorGenerator>

-- except that absent getFITSDescriptor you'd write something like::

  <code>
    return ProductDescriptor.fromAccref(pubDID, 
      "/".join(pubDID.split("/")[4:]))
  </code>

as explained above.


For spectral data model datalink services based on SSAP services, you'll
have to tell the normal sdm generator that you want a different
descriptor class.  This could look like this::

  <descriptorGenerator procDef="//soda#sdm_genDesc">
    <setup>
      <code>
        class PCSLGDescriptor(ssap.SSADescriptor):
          def __init__(self, *args, **kwargs):
            kwargs["accessPath"] = os.path.join(
              base.getConfig("inputsDir"), kwargs["accref"])
            ssap.SSADescriptor.__init__(self, *args, **kwargs)
      </code>
    </setup>
    <bind name="ssaTD">"\rdId#data"</bind>
    <bind name="descriptorClass">PCSLGDescriptor</bind>
  </descriptorGenerator>


Meta Makers
'''''''''''

Meta makers (see `element metaMaker`_) contain code that produces pieces
of service metadata from a data descriptor.  All meta makers belonging
to a service are unconditionally executed, and all must be generator
bodies (i.e., contain a yield statement).

Meta makers may yield input keys (``InputKey`` instances) and/or 
link definitions (``LinkDef`` instances).  The input
keys make up a service's interface in the usual way.

The classes usually required to build input keys return (InputKey,
Values, Option) are available to the code as local names.  There is also
LinkDef, but there is not usually a good reason to use it directly.

As usual, DaCHS structs – that's InputKey, Values, and Option here –
should not be constructed directly but only using the ``MS`` helper
(which is really an alias for base.makeStruct; it takes care that the
special postprocessing of DaCHS structures takes place).

Parameter Definitions
.....................

Hence, a meta maker that generates SSA cutout parameters could look like
this::

  <metaMaker>
    <setup>
      <code>
        parSTC = stc.parseQSTCS('SpectralInterval "LAMBDA_MIN" "LAMBDA_MAX"')
      </code>
    </setup>
    <code>
      for ik in genLimitKeys(MS(InputKey, name="LAMBDA",
        unit="m", stc=parSTC, ucd="em.wl", 
        description="Spectral cutout interval",
        values=MS(Values, 
          min=descriptor.ssaRow["ssa_specstart"],
          max=descriptor.ssaRow["ssa_specend"]))):
        yield ik
    </code>
  </metaMaker>

(something like this is part of the ``//soda#sdm_cutout`` predefined
stream).

The example shows two general techniques for "physical" parameters.
For one, it defines an STC structure.  This is again the "quoted STC" as
discussed in `the DaCHS tutorial`_.  It is a good idea to create the STC
structure in the setup code since parsing STC-S can be relatively CPU 
intensive.  The STC structure resulting from should then be passed as
the ``stc`` keyword parameter to each input key mentioned in the STC
clause.

The second typical technique is the use of the ``genLimitKeys``
function.  This takes a "template" key specifying names, units, and
everything else that can be generically specified, and returns a
sequence of input keys for the limits (i.e., minimal and maximal value
for this).  You'll almost always want this when accepting floating-point
valued parameters, as matching these exactly is at least tricky and
rarely useful.

If the thing you are matching against actually is a column in a database
table, it is usually a good idea to build the input key from the column,
much like with the original mechanism in condition descriptors.  In
python code, this looks like this::

  <metaMaker>
    <code>
      baseIK = InputKey.fromColumn(
        rd.getById("orders").getColumnByName("ecorder")
       ).change(
        values=MS(Values, 
          min=descriptor.ssaRow["order_min"],
          max=descriptor.ssaRow["order_max"]))
      for ik in genLimitKeys(baseIk):
        yield ik
    </code>
  </metaMaker>

This takes input key metadata from the column ecorder in the table
orders.  The change method can take additional keyword/value pairs to
change further properties.

When publishing FITS cubes, you will usually use the
`//soda#fits_makeWCSParams`_ meta maker; it accepts similar QSTCS
specifications as well.  To find out what parameter names the individual
axes are mapped to, first use makeWCSParams without the STC metadata::

   <service id="d" allowed="dlmeta,dlget,form">
    <datalinkCore>
      <descriptorGenerator procDef="//soda#fits_genDesc"/>
      <metaMaker procDef="//soda#fits_makeWCSParams"/>
    </datalinkCore>
  </service>

Then have a look at the metadata produced for a file.  Unless you did
something special, to do that you can just take the accref of a file
from the table containing the products; if the source table was
``mlqso.cubes``, you could figure one out via::

  select accref from dc.products where sourcetable='mlqso.cubes' limit 1

(talk to postgres directly for this query, dc.products is not available
via TAP).

The standard pubDID (as assigned using the ``getStandardPubDID``
rowmaker function) uses your datacenter authority (as configured in
/etc/gavo.rd, when you forget it you can also figure it out by using
``gavo config ivoa authority``) and this accref like this::

  ivo://<authority>/~?<accref>

Hence, to retrieve the datalink document for
``mlqso/data/FBQ0951_data.fits`` on the server dc.g-vo.org using the
datalink renderer on the ``mlqso/q/d`` service, you'd write::

  curl -DID=ivo://org.gavo.dc/~?mlqso/data/FBQ0951_data.fits \
    http://dc.g-vo.org/mlqso/q/d/dlmeta | xmlstarlet fo

(of course, ``xmlstarlet`` isn't actually necessary, and you can use
``wget`` if you want, but you get the idea).

In there you'll see the parameter names for the axes, e.g.,::

  $ curl -s -FID="ivo://org.gavo.dc/~?mlqso/data/FBQ0951_data.fits" \
  >> http://dc.g-vo.org/mlqso/q/d/dlmeta \
  >> | xmlstarlet sel -N v=http://www.ivoa.net/xml/VOTable/v1.2 -T \
  >> -t -m "//v:PARAM" -v "@name" -nl
  serviceAccessURL
  ID
  DEC_MIN
  DEC_MAX
  RA_MIN
  RA_MAX
  WAVELEN_1_MIN
  WAVELEN_1_MAX

If the image is calibrated using a catalog on ICRS, with the wavelength
given as measured, change the ``fits_makeWCSParams`` call to::

  <metaMaker procDef="//soda#fits_makeWCSParams>
    <setup>
      <bind key="stcs"
        >('PositionInterval ICRS "RA_MIN" "DEC_MIN" "RA_MAX" "DEC_MAX"\n'
          'SpectralInterval TOPOCENTER "WAVELEN_1_MIN" "WAVELEN_1_MAX"')
       </bind>
    </setup>
  </metaMaker>

The effect should be a group like::

    <GROUP utype="stc:CatalogEntryLocation">
      <PARAM arraysize="*" datatype="char" 
        name="CoordFlavor" 
        utype="stc:AstroCoordSystem.SpaceFrame.CoordFlavor" value="SPHERICAL"/>
      <PARAM arraysize="*" datatype="char" 
        name="CoordRefFrame" 
        utype="stc:AstroCoordSystem.SpaceFrame.CoordRefFrame" value="ICRS"/>
      <PARAM arraysize="*" datatype="char" 
        name="ReferencePosition" 
        utype="stc:AstroCoordSystem.SpectralFrame.ReferencePosition" 
        value="TOPOCENTER"/>
      <PARAM arraysize="*" datatype="char" name="URI" 
        utype="stc:DataModel.URI" 
        value="http://www.ivoa.net/xml/STC/stc-v1.30.xsd"/>
      <PARAMref ref="apausoh" 
        utype="stc:AstroCoordArea.Position2VecInterval.HiLimit2Vec.C1"/>
      <PARAMref ref="aedwpnn" 
        utype="stc:AstroCoordArea.Position2VecInterval.HiLimit2Vec.C2"/>
      <PARAMref ref="asausoh" 
        utype="stc:AstroCoordArea.Position2VecInterval.LoLimit2Vec.C1"/>
      <PARAMref ref="ahgwpnn" 
        utype="stc:AstroCoordArea.Position2VecInterval.LoLimit2Vec.C2"/>
      <PARAMref ref="ahiusoh" 
        utype="stc:AstroCoordArea.SpectralInterval.HiLimit"/>
      <PARAMref ref="aeiusoh" 
        utype="stc:AstroCoordArea.SpectralInterval.LoLimit"/>
    </GROUP>

All this is explained in [VOTSTC]_.


Link Definitions
.................

Link definitions are usually created using the ``makeLink`` or
``makeLinkFromFile`` methods of descriptor (they are available if the
descriptor class was derived from ``datalink.ProductDescriptor``, as it
usually should).

These functions take a link as the first argument.  The rest are keyword
arguments corresponding to the datalink columns, viz., 

description
  A human-readable short information what's behind the link
semantics
  A term from a controlled-vocabulary describing what's behind the link
  (see below)
contentType
  An (advisory) media type of whatever accessURL points to.  Please make
  sure it's consistent with what the server actually returns if the
  protocol used by accessURL supports that.
contentLength
  The (approximate) size of the resource at accessURL, in bytes 
  (not for ``makeLinkFromFile``, which takes it from the file system)

``makeLinkFromFile`` additionally allows an argument service (see below).

With the exception of semantics, all auxillary data defaults to None if
not given, and it's legal to leave it at that.  Semantics must be
non-NULL, even if an error message is generated.  To make sure that's
true, DaCHS inserts a non-informational URL, which preferentially
shouldn't escape to the user.  Hence, please set semantics on LinkDefs,
and if possible choose one of the terms given at 
http://www.ivoa.net/rdf/datalink/core

You can inspect the definition of the datalinks table active in your
system by saying::

  gavo admin dumpDF //datalink | less

(the table definition is right at the top).

When returning link definitions, the tricky part mostly is to come up
with the URLs.  Use the ``makeAbsoluteURL`` rowmaker function to make
them from relative URLs; the rest just depends on your URL scheme.  An
example could look like this::

  <metaMaker>
    <code>
      yield descriptor.makeLink(
        makeAbsoluteURL("get/"+descriptor.accref[:-5]+".err.fits"),
        contentType="image/fits", semantics="#error",
        description="Errors for this dataset")
      yield descriptor.makeLink(
        "http://foo.bar/raw/"+descriptor.accref.split("/")[-1],
        contentType="image/fits", semantics="#progenitor",
        description="Un-flatfielded, uncalibrated source data")
    </code>
  </metaMaker>

Data links frequently should expose some data that's not in the product
table (e.g., because you don't want the error files to show up there).
In such cases, a good pattern is to allow a static renderer on the
datalink service and use ``makeLinkFromFile``, which expects file paths
relative to the value of the ``staticData`` property::

  <service id="d" allowed="dlget,dlmeta,static">
    <property name="staticData">data/errors</property>

    <datalinkCore>
      ...
      <metaMaker>
        <code>
          stem = descriptor.accref.split("/")[-1].split("_")[0]
          yield descriptor.makeLinkFromFile("%s_err.fits"%stem),
            contentType="image/fits", semantics="#error")
        </code>
      </metaMaker>
    </datalinkCore>
  </service>

Note, however, that the static renderer does not enforce any access
control.  That means that embargoed files must never be within the
staticData (or they are not embargoed any more...).

Although it is preferable to have this static data on the datalink
service to keep things simple, you can reuse an existing service's
static renderer; you would pass in something like
``service=base.resolveCrossId('otherres/q#staticsvc')`` as an addtional
argument to ``makeLinkFromFile``.


Metadata Error Messages
'''''''''''''''''''''''

Both description generators and meta makers can return (or yield, in the
case of meta makers) error messages instead of either a descriptor or a
link definition.  This allows more fine-tuned control over the messages
generated than raising an exception.

Error messages are constructed using class functions of
``DatalinkFault``, which is visible to both procedure types.  The class
function names correspond to the message types defined in the datalink
spec and match the semantics given there:

* AuthenticationFault
* AuthorizationFault
* NotFoundFault
* UsageFault
* TransientFault
* FatalFault
* Fault

Thus, a descriptor generator could look like this::

  <descriptorGenerator>
    <setup>
      <code>
        class MyCustomDescriptor(ProductDescriptor):
          ...
      </code>
    </setup>
    <code>
      with base.getTableConn() as conn:
        matchingRows = list(conn.queryToDicts(
          "select physPath from schema.myTable where pub_did=%(pubDID)s",
          locals()))
        if not matchingRows:
          return DatalinkFault.NotFoundFault(pubDID,
            "No dataset with this pubDID known here")
        return MyCustomDescriptor.fromFile(matchingRows[0]["physPath"])
    </code>
  </descriptorGenerator>

Where sensible, you should pass (as a keyword argument) semantics (as
for LinkDefs) to the ``DatalinkFault``'s constructor; this would
indicate what kind of link you wanted to create.

Data Functions
''''''''''''''

Data functions (see `element dataFunction`_) generate or manipulate
data.  They see the descriptor and the arguments, parsed according to
the input keys produced by the meta makers, where the descriptor's
``data`` attribute is filled out by the first data function called (the
"generating data function").

As described above, DaCHS does not enforce anything on the ``data``
attribute other than that it's not None after the first data function
has run.  It is the RD author's responsibility to make sure that all
data functions in a given datalink core agree on what ``data`` is.

All code in a request for processed data is also passed the input
parameters as processed by the context grammar.  Hence, the code can
rely on whatever contract is implicit in the context grammar, but not
more.  In particular, a datalink core has no way of knowing what data
functions expects which parameters.  If no value for a parameter was
provided on input, the corresponding value is None but a data function
using it still is called.

An example for a generating data function is ``//soda#generateProduct``,
which may be convenient when the manipulations operate on plain local files;
it basically looks like this::

  <dataFunction>
    <code>
      descriptor.data = products.getProductForRAccref(descriptor.accref)
    </code>
  </dataFunction>

(the actual implementation lets you require certain mime types and is
therefore a bit more complicated).

Another generating data function, this time creating a Data instance
containing a spectral data model-compliant structure, is in
``//soda#sdm_genData`` and looks essentially like this::

  <dataFunction>
    <code>
      from gavo import rscdef
      from gavo.protocols import sdm
      builder = base.resolveCrossId(
        "flashheros/q#buildsdm, rscdef.DataDescriptor)
      descriptor.data = sdm.makeSDMDataForSSARow(descriptor.ssaRow, builder)
    </code>
  </dataFunction>

More on this will be discussed in our section on SDM support.

Filtering data functions should always come with a corresponding
metaMaker.  As an example, continuing the spectral cutout example above,
is again in ``//soda#sdm_cutout``.  It simply looks like this::

  <dataFunction>
    <code>
      if not args.get("LAMBDA_MIN") and not args.get("LAMBDA_MAX"):
        return

      from gavo.protocols import sdm
      sdm.mangle_cutout(
        descriptor.data.getPrimaryTable(),
        args["LAMBDA_MIN"] or -1, args["LAMBDA_MAX"] or 1e308)
    </code>
  </dataFunction>

There are situations in which a data function must shortcut, mostly
because it is doing something other than just "pushing on"
descriptor.data.  Examples include preview producers or a data function
that returns the a FITS header.  For cases like this, data functions can
raise one of DeliverNow (which means ``descriptor.data`` must be
something servable, see `Data Formatters`_ and causes that to be
immediately served) or FormatNow (which immediately goes to the data
formatter; this is less useful).

Here's an example for FormatNow; a similar thing is contained in the
STREAM ``//soda#fits_genKindPar``::

  <dataFunction>
    <setup>
      <code>
        from gavo.utils import fitstools
      </code>
    </setup>
    <code>
      if args["KIND"]=="HEADER":
        descriptor.data = ("application/fits-header", 
          fitstools.serializeHeader(descriptor.data[0].header))
        raise DeliverNow()
    </code>
  </dataFunction>



Data Formatters
'''''''''''''''

Data formatters (see `element dataFormatter`_) take a descriptor's data
attribute and build something serveable out of it.  Datalink cores do
not absolutely need one; the default is to return ``descriptor.data``
(the ``//soda#trivialFormatter``, which might be fine if that data is
serveable itself).

What is serveable?  The easiest thing to come up with is a pair of
content type and data in byte strings; if ``descriptor.data`` is a Table
or Data instance, the following could work::

  <dataFormatter>
    <code>
      from gavo import formats

      return "text/plain", formats.getAsText(descriptor.data)
    </code>
  </dataFormatter>
 

Another example is an excerpt from ``//soda#sdm_cutout``::

  <dataFormatter>
    <code>
      from gavo.protocols import sdm

      if len(descriptor.data.getPrimaryTable().rows)==0:
        raise base.ValidationError("Spectrum is empty.", "(various)")

      return sdm.formatSDMData(descriptor.data, args["FORMAT"])
    </code>
  </dataFormatter>

(this goes together with a metaMaker for an input key describing
FORMAT).

An alternative is to return something that has a ``renderHTTP(ctx)``
method that works in nevow.  This is true for the Product instances that
``//soda#generateProduct`` generates, for example.  You can also
write something yourself by inheriting from
protocols.products.ProductBase and overriding its iterData method.

If you don't inherit from ProductBase, take care that this renderHTTP
runs in the main server loop.  If it blocks, the server blocks, so make
sure that this doesn't happen.  The conventional way would be to return,
from the renderHTTP method, some twisted producer.  Non-Product nevow
resources will also not work with asynchronous datalink at this point.


Registry Matters
''''''''''''''''

You can publish the metadata generating endpoint on your service by
saying ``<publish render="dlmeta" sets="ivo_managed"/>``.  However, that
is not recommended, as it clutters the registry with services that
are not really usable after discovery.

An alternative is to add the capability to the service that houses the
discovered datasets.  TODO: Tell people how :-)


Datalink and Obscore
''''''''''''''''''''

In particular for larger datasets like cubes, it is rude to put the
entire dataset into an obscore table.  Although obscore gives expected
download sizes, clients nevertheless do not usually expecte to have to
retrieve several gigabytes or even terabytes of data when dereferencing
an obscore access URL.

Instead, the access URL in the obscore table can point to a datalink
service.  There are various ways to effect this, but the recommended one
is to precompute datalink URLs in the embedding table and then pass
that column to obscore.  This entails defining a suitable column, which
could look like this::

  <column name="dlurl" type="text"
    ucd="meta.ref.url"
    tablehead="DL"
    description="URL of a datalink document for this dataset"
    verbLevel="1" displayHint="type=url"/>

Then fill this column in the rowmaker::

  <var key="obs_id">\standardPubDID</var>
    <map key="dlurl">makeAbsoluteURL(
      "\rdId/*dl*/dlmeta?ID="+urllib.quote(@obs_id))</map>

– you'll need to change ``*dl*`` to the id of your datalink service.
It is true that this stores quite a bit of stuff in the database that
could be computed at runtime.  However, even when you have 1e5 datasets,
we'd be only talking of savings of the order of 10 MB, at the cost of a
seriously ugly obscore expression and reduced debuggability.  Having
said that, it wouldn't be too hard to build these URLs in the obscore
mixin.  With these column, however, you can just state::

  <mixin
    accessURL="dlurl"
    size="10"
    mime="'application/x-votable+xml;content=datalink'"
    ... (all the other stuff) ...
    >//obscore#publish</mixin>

This says that what's coming back is going to be about 10k; it's hard to
predict the exact size of a datalink response, and there's no need to
sweat things for a couple of k more or less.  The mime type given here
is defined by Datalink exactly this purpose.  This is non-negotiable if
you want clients to understand your data.


Datalink Examples
'''''''''''''''''

FITS cutout service
...................

A plain FITS cutout service is assembled like this::

  <service id="dl">
    <meta name="title">Generic FITS datalink service</meta>
    <datalinkCore>
      <descriptorGenerator procDef="//soda#fits_genDesc"/>
      <FEED source="//soda#fits_standardDLFuncs"/>
    </datalinkCore>
  </service>

This works for all FITS files in the products table and has no usable
STC metadata.  Good datalink services do better; by giving more
metadata, you of course commit to certain FITS structures, which means
that you should restrict to only those files that actually match your
assumptions.  The easiest way to do this is to structure your input
directories accordingly and then filter early by using fits_genDesc's
``accrefStart`` parameter.  If, for instance, the data processed where
in INPUTSDIR/thisdata/release2, you would give your descriptor generator
as::

  <descriptorGenerator procDef="//soda#fits_genDesc">
    <bind key="accrefPrefix">"thisdata/release2"</bind>
  </descriptorGenerator>

To have physical SODA cutout parameter, you need to explictly designate
the axes (this is done because FITS files cannot be trusted to properly
declare STC on non-spatial axes).  You can still use
``fits_standardDLFuncs``.  In the simplest case, that would be::

   <FEED source="//soda#fits_standardDLFuncs" spectralAxis="3"/>

(you can also pass ``wavelengthUnit`` if that information is missing
from the FITS.

TODO: Custom function, return link to error file.


SSAP auxillary datalink
.......................

Another use for datalink cores in DaCHS is for server-side processing of
spectra.  A typical service there looks like this::

  <service id="ssaaux" allowed="dlmeta,dlget">
    <meta name="title">Datalink service to retrieve individual spectra</meta>
    <datalinkCore>
      <descriptorGenerator procDef="//soda#sdm_genDesc">
        <bind name="ssaTD">"\rdId#slitspectra"</bind>
      </descriptorGenerator>
      <dataFunction procDef="//soda#sdm_genData">
        <bind name="builder">"\rdId#get_slitcomponent"</bind>
      </dataFunction>
      <FEED source="//soda#sdm_plainfluxcalib"/>
      <FEED source="//soda#sdm_cutout"/>
      <FEED source="//soda#sdm_format"/>
    </datalinkCore>
  </service>

For SDM processing, the descriptor contains the SSA row, and thus
the descriptor generator needs to know the SSA table that keeps the
spectra to be processed.  It is here identified using a full reference
(i.e., including the RD id) to the table definition, in the ssaTD
parameter of ``sdm_genData``; this must of course match whatever is the
``queriedTable`` of the core of the SSA service that refers to this
datalink service. 

The ``sdm_genData`` data function should again cover most uses of this.
Its parameter, however, is somewhat more involved.  The data attribute
of SDM descriptors contains SDM compliant data items, which need to be
created using appropriate RD ``data`` elements.  Such a ``data`` element
needs to be referenced in the ``builder`` parameter of ``sdm_genData``,
again including the RD in the reference.

To define this ``builder``, you first need to define an instance table.
The columns that are in there depend on your data.  In the simplest
case, the ``//ssap#sdm-instance`` mixin is sufficient and bring columns
name ``flux`` and ``spectral``. Here's how you'd add flux errors if you
needed to::

  <table id="instance" onDisk="False">
    <mixin ssaTable="slitspectra"
      spectralDescription="Wavelength"
      fluxDescription="Flux"
      >//ssap#sdm-instance</mixin>

    <meta name="description">A spectrum from a slit spectrum obtained
      for systems of quasars and lensing galaxies.  See 
      ivo://org.gavo.dc/mlqso/q/q</meta>

    <column name="fluxerror" ucd="stat.error;phot.flux.density;em.wl"/>
  </table>

What's referenced in the datalink core's builder data function then is a
``data`` element that builds this table.  Here's one that fills the table
from the database::

  <data id="get_slitcomponent">
    <!-- datamaker to pull spectra values out of the database -->
    <embeddedGrammar>
      <iterator>
        <code>
          obsId = self.sourceToken["accref"].split("/")[-1]
          with base.getTableConn() as conn:
            for row in conn.queryToDicts(
                "SELECT lambda as spectral, flux, error as fluxerror"
                " WHERE obsId=%(obsid)s ORDER BY lambda"):
              yield row
        </code>
      </iterator>
     </embeddedGrammar>
    <make table="instance">
       <parmaker>
         <apply procDef="//ssap#feedSSAToSDM"/>
       </parmaker>
    </make>
  </data>

The ``parmaker`` with the ``//ssap#feedSSAToSDM`` call is generic.  You
will in general have to write custom code for the embedded grammar; just
yield rows matching the instance table.


Product Previews
================

DaCHS has built-in machinery to generate previews from normal, 2D FITS
and JPEG files, where these are versions of the original dataset scaled
to be about 200 pixels in width, delivered as JPEG files.  These
previews are shown on mousing over product links in the web interface,
and they turn up as preview links in datalink interfaces.  This
also generates previews for cutouts.

For any other sort of data, DaCHS does not automatically generate
previews.  To still provide previews – which is highly recommended –
there is a framework allowing you to compute and serve out custom
previews.  This is based on the ``preview`` and ``preview_mime`` columns
which are usually set using parameters in ``//products#define``.

You could use external previews by having http (or ftp) URLs, which
could look like this::

  <rowfilter procDef="//products#define">
    ...
    <bind key="preview">("http://example.org/previews/"
      +"/".join(\inputRelativePath.split("/")[2:]))</bind>
    <bind key="preview_mime">"image/jpeg"/bind>
  </rowfilter>

(this assumes takes away to path elements from the relative paths, which
typically reproduces an external hierachy).  If you need to do more
complex manipulations, you can have a custom rowfilter, maybe like
this if you have both FITS files (for which you want DaCHS' default
behaviour selected with ``AUTO``) and ``.complex`` files with some
external preview::

  <rowfilter name="make_preview_paths">
    <code>
      srcName = os.path.basename(rowIter.sourceToken)
      if srcName.endswith(".fits"):
        row["preview"] = 'AUTO'
        row["preview_mime"] = None
      else:
        row["preview"] = ('http://example.com/previews'
          +os.path.splitext(srcName)[0]+"-preview.jpeg")
        row["preview_mime"] = 'image/jpeg'
      yield row
    </code>
  </rowfilter>
  <rowfilter procDef="//products#define">
    ...
    <bind key="preview">@preview</bind>
    <bind key="preview_mime">@preview_mime</bind>
  </rowfilter>

More commonly, however, you'll have local previews.  If they already
exist, use a static renderer and enter full local URLs as above.

If you don't have pre-computed previews, let DaCHS handle them for you.
You need to do three things:

a) define where the preview files are.  This happens via a
   ``previewDir`` property on the importing data descriptor, like this::

    <data id="import">
      <property key="previewDir">previews</property>
      ...
b) say that the previews are standard DaCHS generated in the
   ``//products#define`` rowfilter.  The main thing you have to
   decide here is the MIME type of the previews you're generating
   (i.e., use the ``standardPreviewPath`` macro unless you know what
   you are doing)::

      <rowfilter procDef="//products#define">
        <bind name="table">"\schema.data"</bind>
        <bind name="mime">"image/fits"</bind>
        <bind name="preview_mime">"image/jpeg"</bind>
        <bind name="preview">\standardPreviewPath</bind>
      </rowfilter>
    
   
c) actually compute the previews.  This is usually not defined in the RD
   but rather using DaCHS' processing framework. `Precomputing
   previews`_ in the processor documentation covers this in more detail;
   the upshot is that this can be as simple as::

     from gavo.helpers import processing

     class PreviewMaker(processing.SpectralPreviewMaker):
       sdmId = "build_sdm_data"

     if __name__=="__main__":
       processing.procmain(PreviewMaker, "flashheros/q", "import")



.. _precomputing previews: http://docs.g-vo.org/DaCHS/processors.html#precomputing-previews


Writing Custom Cores
====================

While DaCHS provides cores for many common operations -- in particular,
database queries and wrapped external binaries --, there are of course
services needing to do things not covered by what the shipped cores do.
Some such cases still follow the basic premise of services: GET or POST
parameters in, something table-like out.  For these cases, use custom
cores (if even this does not provide sufficent functionality, write a
custom renderer).

For simple cases, rather than having the code in a separate module it's
nicer to keep everything together in the RD.  This is very similar;
`Python Cores instead of Custom Cores`_ explains the differences.



Defining a Custom Core
''''''''''''''''''''''

To do this, you need to write a python module.  The standard location
for those is in the bin/ subdirectory of the resource directory.

You will usually want to inherit from core::

  from gavo import rsc
  from gavo.svcs import core

  class Core(core.Core):

The framework will always look of an object named "Core" in the module
and use this as the custom core.

The core needs an InputTable and an OutputTable like all cores.  You
*could* define it in the resource descriptor like this::

  <customCore id="createCore" module="bin/create">
    <inputTable>
      <inputKey .../>
    </inputTable>
    <outputTable>
      <column name="itemsAdded" type="integer" tablehead="Items added"/>
    </outputTable>
  </customCore>

It's probably a better idea to define it in the code, though, since
then it will work without further specifications.  The definitions
in the code can still be overridden from an RD for special effects.
Embedding the definitions is done using the class attributes
``inputTableXML`` and ``outputTableXML``::

  class Core(core.Core):
    inputTableXML = """<inputTable>
      <inputKey name="fileSrc" type="file" tablehead="Local file"
        description="A local file to upload (overrides source URL if given).">
      <inputKey name="tableName" type="text" tablehead="Target Table"
        description="Name of the table to match against.  
          Only tables available for ADQL (see there) can be used here.">
        <values fromdb="tablename from dc_tables where adql=True"/>
      </inputTable>
      """
    outputTableXML = """<outputTable/>"""

You should not override the constructor.  If you need to perform
"expensive" instanciations, override the completeElement method, as in
the following template::

  def completeElement(self):
    <your code>
    self._completeElementNext(Core)

The call to _completeElementNext ensures that the remaining
completeElement methods are executed.

Giving the Core Functionality
'''''''''''''''''''''''''''''

To have the core do something, you have to override the run method,
which has to have the following signature::

  run(service, inputTable, queryMeta) -> stuff

The stuff returned will ususally be a Table instance (that need not
match the outputTable definition -- the latter is targetted at the
registry and possibly applications like output field selection).  The
standard renderers also accept a mime type and a string containing
some data and will deliver this as-is.  With custom renderers, you could
return basically anything you want.

Services come up with some idea of the schema of the table they want to
return and adapt tables coming out of the core to this.  Sometimes, you
want to suppress this behaviour, e.g., because the service's ideas are
off.  In that case, set a noPostprocess atttribute on the table to any
value.

service is a service instance.  In particular, you can access the RD you
are running in through its rd attribute.  This is useful if you need to
resolve, e.g., table references (which, in this case, could be given as
a service property)::

  pertainingTable = service.rd.getById(
    service.getProperty("pertainingTable"))

inputTable is a Table instance. Unless the service has a fancy inputDD,
you simply find the inputKey values in the table's parameters::

  val = inputTable.getParam("fileSrc")

This val will be a simple value if the ``inputKey`` has multiplicity
``single`` or ``force-single``, a list if it has multiplicity
``multiple``.  It will be None if a non-required parameter is missing.

As said above, you could return finished documents from your custom
core.  It's usually friendlier and more flexible if you return
tables.  To build a table matching the output table, use the core's
``outputTable`` (which is actually a table definition).  This could
look like this::

  res = rsc.TableForDef(self.outputTable)
  res.addRow({"foo": 3, "bar": 8})

The rows are rowdicts as genrated by rowmakers (which you could
use, too).  In many cases, it may be more convenient to collect the data
up front and then create the output table with ready-made rows::

  rows = [{"foo": 3, "bar": 8}]
  return rsc.TableForDef(self.outputTable, rows=rows)

Lastly, if there's parsing involved in coming up with the output table,
it's probably a good idea to simply write a normal data item and arrange
for whatever you come up with to be parsed.  For a data item
``resparser``, this would then look like this::

  with open("myFile") as src:
    return rsc.makeData(rd.getById("resparser"), forceSource=src)



Errors
''''''

To bail out from processing, raise a validation error.  Construct it
with a message and the name of an input key.  At least for the form
renderer, this causes a sensible error message with some hint as the
originating input field, next to the offending input field::

  raise base.ValidationError("Invalid file name", "rdsrc")


Database Options
''''''''''''''''

The standard DB cores receive a "table widget" on form generation,
including sort and limit options.  To make the Form renderer output this
for your core as well, define a method wantsTableWidget() -> True.

The queryMeta that you receive in run has a dbLimit key.  It contains
the user selection or, as a fallback, the global db/defaultLimit value.
These values are integers.

So, if you order a table widget, you should do something like::

  cursor.execute("SELECT .... LIMIT %(queryLimit)s", 
    {"queryLimit": queryMeta["dbLimit"],...})

In general, you should warn people if the query limit was reached; a
simple way to do that is::

  if len(res)==queryLimit:
    res.addMeta("_warning", "The query limit was reached.  Increase it"
      " to retrieve more matches.  Note that unsorted truncated queries"
      " are not reproducible (i.e., might return a different result set"
      " at a later time).")

where res would be your result table.  _warning metadata is displayed in
both HTML and VOTable output, though of course VOTable tools will not
usually display it.

Inheriting from TableBasedCore
''''''''''''''''''''''''''''''

TBD (This does not work right now; complain if you need to do it)

Python Cores instead of Custom Cores
''''''''''''''''''''''''''''''''''''

If you only have a couple of lines of python, you don't have to have a
separate module.  Instead, use a python core.  In it, you essentially
have the ``run`` method as discussed in `Giving the Core Functionality`_
in a standard ``procApp``.  The advantage is that interface and
implementation is nicely bundled together.  The following example should
illustrate the use  of such python cores; note that ``rsc`` already is
in the procApp's namespace::

  <pythonCore>
    <inputTable>
      <inputKey name="opre" description="Operand, real part"
        required="True"/>
      <inputKey name="opim" description="Operand, imaginary part"
        required="True"/>
      <inputKey name="powers" description="Powers to compute"
        type="integer" multiplicity="multiple"/>
    </inputTable>
    <outputTable>
      <outputField name="re" description="Result, real part"/>
      <outputField name="im" description="Result, imaginary part"/>
      <outputField name="log"
        description="real part of logarithm of result"/>
    </outputTable>

    <coreProc>
      <setup>
        <code>
          import cmath
        </code>
      </setup>
      <code>
        powers = inputTable.getParam("powers")
        if powers is None:
          powers = [1,2]
        op = complex(inputTable.getParam("opre"),
          inputTable.getParam("opim"))

        rows = []
        for p in powers:
          val = op**p
          rows.append({
            "re": val.real,
            "im": val.imag,
            "log": cmath.log(val).real})
        
        return rsc.TableForDef(self.outputTable, rows=rows)
      </code>
    </coreProc>
  </pythonCore>

As an additional service, DaCHS executes your python cores in a sandbox
directory, so you can create temporary files to your heart's
delight; they will be torn down once the core is finished.


Custom UWSes
============

Universal Worker Systems (UWSes) allow the asynchronous operation of
services, i.e., the server runs a job on behalf of the user without the
need for a persistent connection.

DaCHS supports async operations of TAP and datalink out of the box.
If you want to run async services defined by your own code, there are a
few things to keep in mind.

(1) You'll need to prepare your database to keep track of your custom
jobs (just once)::
  
  gavo imp //uws enable_useruws

(2) You'll have to allow the ``uws.xml`` renderer on the service in
question.

(3) Things running within a UWS are fairly hard to debug in DaCHS right
now.  Until we have good ideas on how to make these things a bit more
accessible, it's a good idea to at least for debugging also allow
synchronous renderers, for instance, ``form`` or ``api``.  If something
goes wrong, you can do a sync query that then drops you in a debugger
in the usual manner (see the debugging chapter in the tutorial).

(4) For now, the usual queryMeta is not pushed into the uws handler
(there's no good reason for that).  We do, however, transport on
DALI-type RESPONSEFORMAT.  To enable that on automatic results (see
below), say::

  <inputKey name="responseformat" description="Preferred
    output format" type="text"/>

in your input table.

(5) All UWS parameters are lowercased and only available in lowercased
form to server-side code.  To allow cores to run in both sync and async
without further worries, just have lowercase-only parameters.

(6) As usual, the core may return either a pair of (media type, content)
or a data item, which then becomes a UWS result named ``result`` with
the proper media type.  You can also return None (which will make the
core incompatible with most other renderers).  That may be a smart thing
to do if you're producing multiple files to be returned through UWS.  To
do that, there's a ``job`` attribute on the inputTable that has an
``addResult(source, mediatype, name)`` method.  Source can be a string
(in which case the string will be the result) or a file open for reading
(in which case the result will be the file's content).  Input tables
of course don't have that attribute unless they come from the uws
rendererer.  Hence, a typical pattern to use this would be::

  if hasattr(inputTable, "job"):
    with inputTable.job.getWritable() as wjob:
      wjob.addResult("Hello World.\\n", "text/plain", "aux.txt")

or, to take the results from a file that's already on-disk::

  if hasattr(inputTable, "job"):
    with inputTable.job.getWritable() as wjob:
      with open("other-result.txt") as src:
        wjob.addResult(src, "text/plain", "output.txt")

Right now, there's no facility for writing directly to UWS result files.
Ask if you need that.

(7) UWS lets you add arbitrary files using standard DALI-style uploads.
This is enabled if there are ``file``-typed inputKeys in the service's
input table.  These inputKeys are otherwise ignored right now.
See [DALI]_ for details on how these inputs work.  To create an
inline upload from a python client (e.g., to write a test), it's most
convenient to use the requests package, like this::

  import requests

  requests.post("http://localhost:8080/data/cores/pc/uws.xml/D2hFEJ/parameters",
    {"UPLOAD": "stuff,param:upl"},
    files = {"upl": open("zw.py")})

From within your core, use the file name (the name of the input key) and
pull the file from the UWS working directory::

  with open(os.path.join(inputTable.job.getWD(), "mykey")) as f:
    ...


Hint on debugging: ``gavo uwsrun`` doesn't check the state the job is
in, it will just try to execute it anyway.  So, if your job went into
error and you want to investicate why, just take its id and execute
something like::

  gavo --traceback uwsrun i1ypYX


Custom Pages
============

While DaCHS isn't actually intended to be an all-purpose server for web
applications, sometimes you want to have some gadget for the browser
that doesn't need VO protocols.  For that, there is customPage, which is
essentially a bare-bones nevow page.  Hence, all (admittedly sparse)
nevow documentation applies.  Nevertheless, here are some hints on how
to write a custom page.

First, in the RD, define a service allowing a custom page.  These
normally have no cores (the customPage renderer will ignore the core)::

  <service id="ui" core="null" allowed="custom" 
    customPage="res/registration.py">
    <meta name="shortName">DOI registration</meta>
    <meta name="title">VOiDOI DOI registration web service</meta>
  </service>

The python module referred to in customPage must define a ``MainPage``
nevow resource.  The recommended pattern is like this::

  from nevow import tags as T

  from gavo import web
  from gavo.imp import formal


  class MainPage(
      formal.ResourceMixin, 
      web.CustomTemplateMixin,
      web.ServiceBasedPage):

    name = "custom"
    customTemplate = "res/registration.html"

    workItems = None

    @classmethod
    def isBrowseable(self, service):
      return True

    def form_ivoid(self, ctx, data={}):
      form = formal.Form()
      form.addField("ivoid", formal.String(required=True), label="IVOID",
        description="An IVOID for a registred VO resource"),
      form.addAction(self.submitAction, label="Next")
      return form

    def render_workItems(self, ctx, data):
      if self.workItems:
        return ctx.tag[T.li[[m for m in self.workItems]]]
      return ""

    def submitAction(self, ctx, form, data):
      self.workItems = ["Working on %s"%data["ivoid"]]
      return self
      
The ``formal.ResourceMixin`` lets you define and interpret forms.  The
``web.ServiceBasedPage`` does all the interfacing to the DaCHS (e.g.,
credential checking and the like).  The ``web.CustomTemplateMixin`` lets
you get your template from a DaCHS template (cf. `templating guide`_)
from a resdir-relative directory given in the ``customTemplate``
attribute.  For widely distributed code, you should additionaly provide
some embedded stan fallback in the ``defaultDocFactory`` attribute -- of
course, you can also give the template in stan in the first place.

On ``form_invoid`` and ``submitAction`` see below.

This template could, for this service, look like this::

  <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
      "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

  <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:n="http://nevow.com/ns/nevow/0.1">
  <head>
    <title>VOiDOI: Registration</title>
    <n:invisible n:render="commonhead"/>
  </head>
  <body n:render="withsidebar">
    <h1>VOiDOI: Register your VO resource</h1>
    <ul n:render="workItems"/>
    <p>VOiDOI lets you obtain DOIs for registered VO services.</p>

    <p>In the form below, enter the IVOID of the resource you want a DOI for.
    If the resource is known to our registry but has no DOI yet, the registred
    contact will be sent an e-mail to confirm DOI creation.</p>
    <n:invisible n:render="form ivoid"/>
  </body>
  </html>

Most of the details are explained in the `templating guide`_.  The
exception is the ``form ivoid``.  This makes the
``formal.ResourceMixin`` call the ``form_ivoid`` in ``MainPage`` and put
in whatever HTML/stan that returns.  If nevow detects that the request
already results from filling out the form, it will execute what your
registred in ``addAction`` -- in this case, it's the ``submitAction``
method.

*Important*: anything you do within ``addAction`` runs within the
(cooperative) server thread.  If it blocks or performs a long
computation, the server is blocked.  You will therefore want to do
non-trivial things either using asynchronous patterns or using
``deferToThread``.  The latter is less desirable but also easier, so
here's how this looks like::

  def submitAction(self, ctx, form, data):
    return threads.deferToThread(
      runRegistrationFor, data["ivoid"]
      ).addCallback(self._renderResponse
      ).addErrback(self._renderErrors)
  
  def _renderResponse(self, result):
    # do something to render a success message (or return Redirect)
    return self

  def _renderErrors(self, failure):
    # do something to render an error message, e.g., from
    # failure.getErrorMessage()
    return self

The embedding RD is available in the custom pages's global namespace as
``RD``.  Thus, the standard pattern for creating a read only table is::
  
  with api.getTableConn() as conn: table =
    api.TableForDef(RD.getById("my_table"), connection=conn)

If you need write access, you would write::

  with api.getWritableAdminConn() as conn:
    table = api.TableForDef(RD.getById("my_table"), connection=conn)

The ``RD`` attribute is *not* avalailable during module import.  This is
a bit annoying if you want to load resources from an RD-dependent place;
this, in particular, applies to importing dependent modules.  To provide
a workaround, DaCHS calls a method ``initModule(**kwargs)`` after
loading the module.  You should accept arbitrary keyword arguments here
so you code doesn't fail if we find we want to give ``initModule`` some
further information.

The common case of importing a module from some RD-dependent place thus
becomes::

  from gavo import utils

  def initModule(**kwargs):
    global oai2datacite
    modName = RD.getAbsPath("doitransfrom/oai2datacite")
    oai2datacite, _ = utils.loadPythonModule(modName)


.. _templating guide: http://docs.g-vo.org/DaCHS/templating.html

Manufacturing Spectra
=====================

TODO: Update this for Datalink

Making SDM Tables
'''''''''''''''''

Compared to images, the formats situation with spectra is a mess.
Therefore, in all likelihood, you will need some sort of conversion
service to VOTables compliant to the spectral data model.  DaCHS has a
facility built in to support you with doing this on the fly, which means
you only need to keep a single set of files around while letting users
obtain the data in some format convenient to them.  The tutorial
contains examples on how to generate metadata records for such
additional formats.

First, you will have to define the "instance table", i.e., a table
definition that will contain a DC-internal representation of the
spectrum according to the data model.  There's a mixin for that::

  <table id="spectrum">
    <mixin ssaTable="hcdtest">//ssap#sdm-instance</mixin>
  </table>

In addition to adding lots and lots of params, the mixin also defines
two columns, ``spectral`` and ``flux``; these have units and ucds as
taken from the SSA metadata.  You can add additional columns (e.g., a
flux error depending the the spectral coordinate) as requried.

The actual spectral instances can be built by sdmCores and delivered
through DaCHS' product interface.  Note, however, that clients
`supporting getData`_ wouldn't need to do this.  You'll still have to
define the data item defined below.

sdmCores, while potentially useful with common services, are intended to
be used by the product renderer for dcc product table paths.  They
contain a data item that must yield a primary table that is basically
sdm compliant.  Most of this is done by the //ssap#feedSSAToSDM apply
proc, but obviously you need to yield the spectral/flux pairs (plus
potentially more stuff like errors, etc, if your spectrum table has more
columns.  This comes from the data item's grammar, which probably must
always be an embedded grammar, since its sourceToken is an SSA row in a
dictionary.  Here's an example::

  <sdmCore queriedTable="hcdtest" id="mksdm">
    <data id="getdata">
      <embeddedGrammar>
        <iterator>
          <code>
            labels = ("spectral", "flux")
            relPath = self.sourceToken["accref"].split("?")[-1]
            with self.grammar.rd.openRes(relPath) as inF:
              for ln in inF:
                yield dict(zip(labels,ln.split()))
          </code>
        </iterator>
      </embeddedGrammar>
      <make table="spectrum">
        <parmaker>
          <apply procDef="//ssap#feedSSAToSDM"/>
        </parmaker>
      </make>
    </data>
  </sdmCore>

Note: spectral, flux, and possibly further items coming out of the
iterator must be in the units units promised by the SSA metadata
(fluxSI, spectralSI).  Declarations to this effect are generated by the
``//ssap#sdm-instance`` mixin for the spectral and flux columns.

The sdmCores are always combined with the sdm renderer.  It passes an
accref into the core that gets turned into an row from queried table;
this must be an "ssa" table (i.e., right now something that mixes in
``//ssap#hcd``).  This row is the input to the embedded data descriptor.
Hence, this has no sources element, and you must have either a custom
or embedded grammar to deal with this input.


Echelle Spectra
===============

Echelle spectrographs "fold" a spectrum into several orders which may be
delivered in several independent mappings from spectral to flux
coordinate.  In this split form, they pose some extra problems, dealt
with in an extra system RD, ``//echelle``.  For merged Echelle spectra,
just use the standard SSA framework.


Table
'''''

Echelle spectra have additional metadata that should end up in their SSA
metadata table – these are things like the number of orders, the minimum
and maximum (Echelle) order, and the like.  To pull these columns into
your metadata table, use the ssacols stream, for example like this::

  <table id="ordersmeta" onDisk="True" adql="True">
    <meta name="description">SSA metadata for split-order 
      Flash/Heros Echelle spectra</meta>
    <mixin
      [...]
      statSpectError="0.05"
      spectralResolution="2.5e-11"
    >//ssap#hcd</mixin>
    <mixin
      calibLevel="1">//obscore#publishSSAPHCD</mixin>
    <column name="localKey" type="text"
      ucd="meta.id"
      tablehead="Key"
      description="Local observation key."
      verbLevel="1"/>
    <STREAM source="//echelle#ssacols"/>
  </table>



Supporting getData
==================

DaCHS still has support the now-abandoned 2012 getData specification by
Demleitner and Skoda.   If you think you still want this, contact the
authors; meanwhile, you really should be using datalink for whatever you
think you need getData for.



Adapting Obscore
================

You may want extra, locally-defined columns in your obscore tables.  To
support this, there are three hooks in obscore that you can exploit.
The hooks are in ``userconfig.rd`` (see `Userconfig RD in
the operator's guide`_ to where it is and how to get started with it)
It helps to have a brief look at the ``//obscore`` RD (e.g., using
``gavo admin dumpDF //obscore``) to get an idea what these hooks do.

Within the template ``userconfig.rd``, there are already three STREAMs
with ids starting with obscore.; these are referenced from within the
system ``//obscore`` RD.  Here's an somewhat more elaborate example::

  <STREAM id="obscore-extracolumns">
    <column name="fill_factor"
      description="Fill factor of the SED"
      verbLevel="20"/>
  </STREAM>

  <STREAM id="obscore-extrapars">
    <mixinPar name="fillFactor" 
      description="The SED's fill factor">NULL</mixinPar>
  </STREAM>

  <STREAM id="obscore-extraevents">
    <property name="obscoreClause" cumulate="True">
      ,
      CAST(\\\\fillFactor AS real) AS fill_factor
    </property>
  </STREAM>

(to be on the safe side: there need to be four backslashes in front of
fillFactor; this is just a backslash doubly-escaped.  Sorry about this).

The way this is used in an actual mixin would be like this::

  <table id="specs" onDisk="True">
    <mixin ...>//ssap#hcd</mixin>
    <mixin
      ... (all the usual parameters)
      fillFactor="0.3">//obscore#publishSSAPHCD</mixin>
  </table>

What's going on here?  Well, ``obscore-extracolumns`` is easy – this
material is directly inserted into the definition of the obscore view
(see the table with id ``ObsCore`` within the ``//obscore`` RD).  You
could abuse it to insert other stuff than columns but probably should
not.

The tricky part is ``obscore-extraevents``.  This goes into the
``//obscore#_publishCommon`` STREAM and ends up in all the publish
mixins in obscore.  Again, you could insert mixinPars and similar at
this point, but the only thing you really must do is add lines to the
big SQL fragment in the ``obscoreClause`` property that the mixin leaves
in the table.  This is what is made into the table's contribution to the
big obscore union. Just follow the example above and, in particular,
always CAST to the type you have in the metadata, since individual tables
might have NULLs in the values, and you do not want misguided attempts
by postgres to do type inference then.

If you actually must know why you need to double-escape fillFactor and
what the magic with the ``cumulate="True"`` is, ask.

Finally, ``obscore-extrapars`` directly goes into a core component of
obscore, one that all the various publish mixins there use.  Hence, all
of them grow your functionality.  That is also why it is important to
give defaults (i.e., element content) to all mixinPars you give in this
way – without them, all those other publish mixins would fail unless
their applications in the RDs were fixed.

If you change ``%#obscore-extracolumns``, all the statement fragments
contributed by the obscore-published tables need to be fixed.  To spare
you the effort of touching a potentially sizeable number of RDs, there's
a data element in //obscore that does that for you; so, after every
change just run::

  gavo imp //obscore refreshAfterSchemaUpdate

This may fail if you didn't clean up properly after deleting a resource
that once contributed to ivoa.obscore.  In that case you'll see an error
message like::

  *** Error: table u'whatever.main' could not be located in dc_tables

In that case, just tell DaCHS to forget the offending table::

  gavo purge whatever.main

Another problem can arise when a table once was published to obscore but
now no longer is while still existing.  DaCHS in that case will still
have an entry for the table in ivoa._obscoresources, which results in an
error like::

  Table definition of whatever.main> has no property 'obscoreClause' set

The fastest way to fix this situation is to drop the offending line in
the database manually::
  
  psql gavo -c "delete from ivoa._obscoresources where tablename='whatever.main'"

.. _Userconfig RD in the operator's guide: http://docs.g-vo.org/DaCHS/opguide.html#userconfig-rd



Writing Custom Grammars
=======================

A custom grammar simply is a python module located within a resource
directory defining a row iterator class derived from
gavo.grammars.customgrammar.CustomRowIterator; this class must be called
RowIterator.  You want to override the _iterRows method.  It will have
to yield row dictionaries, i.e., dictionaries mapping string keys to
something (preferably strings, but you will usually get away with
returning complete values even without fancy rowmakers).  

So, a custom grammar module could look like this::

  from gavo.grammars.customgrammar import CustomRowIterator

  class RowIterator(CustomRowIterator):
    def _iterRows(self):
      for i in xrange(10000):
        yield {'index': i, 'square': i**2}

Do not override magic methods, since you may lose row filters, sourceFields,
and the like if you do.  An exception is the constructor.  If you must,
you can override it, but you must call the parent constructor, like
this::

  class RowIterator(CustomRowIterator):
    def __init__(self, grammar, sourceToken, sourceRow=None):
      CustomRowIterator.__init__(self, grammar, sourceToken, sourceRow)
      <your code>

The sourceToken, in general, will be a file name, unless you call
makeData manually and forceSource something else.

A row iterator will be instanciated for each source processed.  Thus,
you should usually not perform expensive operations in the constructor
unless they depend on sourceToken.  In general, you should rather define
a function makeDataPack in the module.  Whatever is returned by this
function is available as self.grammar.dataPack in the row iterator.

The function receives an instance of the the customGrammar as an
argument.  This means you can access the resource descriptor and
properties of the grammar.  As an example of how this could be used,
consider this RD fragment::

  <table id="defTable">
    ...
  </table>

  <customGrammar module="res/grammar">
    <property name="targetTable">defTable</property>
  </customGrammar>

Then you could have the following in res/grammar.py::

  def makeDataPack(grammar):
    return grammar.rd.getById(grammar.getProperty("targetTable"))

and access the table in the row iterator.

Also look into EmbeddedGrammar, which may be a more convenient way to
achieve the same thing.

A fairly complex example for a custom grammar is a provisional 
`Skyglow grammar`_ .

.. _Skyglow grammar: http://svn.ari.uni-heidelberg.de/svn/gavo/hdinputs/lightmeter/res/skyglowgrammar.py


Dispatching Grammars
''''''''''''''''''''

With normal grammars, all rows are fed to all rowmakers of all makes
within a data object.  The rowmakers can then decide to not process a
given row by raising ``IgnoreThisRow`` or using the trigger mechanism.
However, when filling complex data models with potentially dozens of
tables, this becomes highly inefficient.

When you write your own grammars, you can to better.  Instead of just
yielding a row from ``_iterRows``, you yield a pair of a role (as
specified in the ``role`` attribute of a ``make`` element) and the row.
The machinery will then pass the row only to the feeder for the table in
the corresponding make.

Currently, the only way to define such a dispatching grammar is to use a
custom grammar or an embedded grammar.  For these, just change your
``_iterRows`` and say ``isDispatching="True"`` in the ``customGrammar``
element.  If you implement ``getParameters``, you can return either
pairs of role and row or just the row; in the latter case, the row will
be broadcast to all parmakers.

Special care needs to be taken when a dispatching grammar parses
products, because the product table is fed by a special make inserted
from the products mixin.  This make of course doesn't see the rows you
are yielding from your dispatching grammar.  This means that without
further action, your files will not end up the the product table at all.
In turn, getproducts will return 404s instead of your products.

To fix this, you need to explicitely yield the rows destined for the 
products table with a products role, from within your grammar.  Where
the grammar yield rows for the table with metadata (i.e., rows that actually
contain the fields with prodtblAccref, prodtblPath, etc), yield
to the products table, too, like this: ``yield ("products", newRow)``.


Functions Available for Row Makers
==================================

In principle, you can use arbitrary python expressions in var, map and
proc elements of row makers.  In particular, the namespace in which
these expressions are executed contains math, os, re, time, and datetime
modules as well as gavo.base, gavo.utils, and gavo.coords.

However, much of the time you will get by using the following functions
that are immediately accessible in the namespace:

.. replaceWithResult getRmkFuncs(docStructure)



Scripting
=========

As much as it is desirable to describe tables in a declarative manner,
there are quite a few cases in which some imperative code helps a lot
during table building or teardown.  Resource descriptors let you embed
such imperative code using script elements.  These are children of the
make elements since they are exclusively executed when actually
importing into a table.

Currently, you can enter scripts in SQL and python, which may be called
at various phases during the import.

SQL scripts
'''''''''''

In SQL scripts, you separate statements with semicolons.  Note that no
statements in an SQL script may fail since that will invalidate the
transaction.  This is a serious limitation since you must not commit or
begin transactions in SQL scripts as long as Postgres  does not support
nested transactions.

You can use table macros in the SQL scripts to parametrize them; the
most useful among those probably is ``\curtable`` containing the fully
qualified name of the table being processed.

Python scripts
''''''''''''''

Python scripts can be indented by a constant amount.

The table object currently processed is accessible as table.  In
particular, you can use this to issue queries using 
``table.query(query, arguments)`` (parallel to dbapi.execute) and to
delete rows using ``table.deleteMatching(condition, pars)``.  The
current RD is accessible as ``table.rd``, so you can access items from
the RD as ``table.rd.getById("some_id")``, and the recommended way to
read stuff from the resource directory is
``table.rd.openRes("res/some_file)``.

Some types of scripts may have additional names available.  Currently,
newSource and sourceDone have the name sourceToken – which is the
sourceToken as passed to the grammar.

Script types
''''''''''''

The type of a script corresponds to the event triggering its execution.
The following types are defined right now:

* preImport -- before anything is written to the table
* preIndex -- before the indices on the table are built
* postCreation -- after the table (incl. indices) is finished
* beforeDrop -- when the table is about to be dropped
* newSource -- every time a new source is started
* sourceDone -- every time a source has been processed

Note that preImport, preIndex, and postCreation scripts are not executed
when a table is updated, in particular, in data items with
``updating="True"``.  The only way to run scripts in such circumstances
is to use newSource and sourceDone scripts.


Examples
''''''''

This snippet sets a flag when importing some source (in this case,
that's an RD, so we can access sourceToken.sourceId::

      <script type="newSource" lang="python" id="markDeleted">
        table.query("UPDATE %s SET deleted=True"
          " WHERE sourceRD=%%(sourceRD)s"%id, 
          {"sourceRD": sourceToken.sourceId})
      </script>


This is a hacked way of ensuring some sort of referential integrity:
When a table containing "products" is dropped, the corresponding entries
in the products table are deleted::

  <script type="beforeDrop" lang="SQL" name="clean product table">
    DELETE FROM products WHERE sourceTable='\curtable'
  </script>

Note that this is actually quite hazardous because if the table is
dropped in any way not using the make element in the RD, this will not
be executed.  It's usually much smarter to tell the database to do the
housekeeping.  Rules are typically set in postCreation scripts::

  <script type="postCreation" lang="SQL">
    CREATE OR REPLACE RULE cleanupProducts AS 
      ON DELETE TO \curtable DO ALSO
      DELETE FROM products WHERE key=OLD.accref
  </script>

The decision if such arrangements are make before the import, before the
indexing or after the table is finished needs to be made based on the
script's purpose.

Another use for scripts is SQL function definition::

      <script type="postCreation" lang="SQL" name="Define USNOB matcher">
        CREATE OR REPLACE FUNCTION usnob_getmatch(alpha double precision, 
          delta double precision, windowSecs float
        ) RETURNS SETOF usnob.data AS $$
        DECLARE
          rec RECORD;
        BEGIN
          FOR rec IN (SELECT * FROM usnob.data WHERE 
            q3c_join(alpha, delta, raj2000, dej2000, windowSecs/3600.)) 
          LOOP
            RETURN NEXT rec;
          END LOOP;
        END;
        $$ LANGUAGE plpgsql;
      </script>

You can also load data, most usefully in preIndex scripts (although
beforeImport would work as well here)::

    <script type="preIndex" lang="SQL" name="create USNOB-PPMX crossmatch">
        SET work_mem=1000000;
        INSERT INTO usnob.ppmxcross (
          SELECT q3c_ang2ipix(raj2000, dej2000) AS ipix, p.localid 
          FROM 
            ppmx.data AS p, 
            usnob.data AS u 
          WHERE q3c_join(p.alphaFloat, p.deltaFloat, 
            u.raj2000, u.dej2000, 1.5/3600.))
    </script>


Embedded Documentation
======================

ReStructuredText
''''''''''''''''

Text needing some amount of markup within DaCHS is almost always
input as ReStructuredText (RST).  The source versions of the 
`DaCHS documentation`_ give examples for such markup, and DaCHS users
should at least briefly skim the `ReStructuredText primer`_.

DaCHS contains some RST extensions.  Some of them are discussed in
`Examples Endpoints`_.  Generally useful extensions include:

bibcode
  This text role formats the argument as a link into ADS when rendered
  as HTML.  For technical reasons, this currently ignores the configured
  ADS mirror and always uses the Heidelberg one.  Complain if this bugs
  you.  To use it, you'd write::
    
    See also :bibcode:`2011AJ....142....3H`.

.. _DaCHS documentation: http://docs.g-vo.org/DaCHS/
.. _ReStructuredText primer: http://docutils.sourceforge.net/docs/user/rst/quickstart.html

Examples Endpoints
''''''''''''''''''

TBD


System Tables
=============

DaCHS uses a number of tables to manage services and implement
protocols.  Operators should not normally be concerned with them, but
sometimes having a glimpse into them helps with debugging.

If you find yourself wanting to change these tables' content, please
post to `dachs-support`_ first describing what you're trying to do.
There should really be commands that do what you want, and it's
relatively easy to introduce subtle problems by manipulating system
tables without going through those.

Having said that, here's a list of the system tables together with brief
descriptions of their role and the columns contained.  Note that your
installation might not have all of those; some only appear after a ``gavo
imp`` of the RD they are defined in -- which you of course only should
do if you know you want to enable the functionality provided.

The documentation given here is extracted from the resource descriptors,
which, again, you can read in source using ``gavo admin dumpDF
//<rd-name>``.

.. replaceWithResult makeSystemTablesList(docStructure)

.. _dachs-support: http://lists.g-vo.org/cgi-bin/mailman/listinfo/dachs-support


Bibliography
============

.. [RMI]  Hanisch, R., et al, "Resource Metadata for the Virtual
   Observatory", http://www.ivoa.net/Documents/latest/RM.html
.. [VOTSTC] Demleitner, M., Ochsenbein, F., McDowell, J., Rots, A.:
   "Referencing STC in VOTable", Version 2.0,
   http://www.ivoa.net/Documents/Notes/VOTableSTC/20100618/NOTE-VOTableSTC-2.0-20100618.pdf
.. _the DaCHS tutorial: http://docs.g-vo.org/DaCHS/tutorial.html
.. [DALI] Dowler, P, et al, "Data Access Layer Interface Version 1.0", http://ivoa.net/documents/DALI/20131129/
.. |date| date::
